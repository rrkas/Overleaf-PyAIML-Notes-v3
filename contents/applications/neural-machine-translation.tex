\chapter{Neural Machine Translation (NMT)}

\section{Intro}

\begin{enumerate}
    \item Translate sentences from one language to another using Neural Networks (NNs)
\end{enumerate}


\subsection{Probabilistic Perspective}

\begin{enumerate}
    \item translation is equivalent to finding a target sentence $y$ that maximizes the conditional probability of $y$ given a source sentence $x$, i.e., $\arg\max_y P(y | x)$.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item  we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}


















\section{Encoder-decoder Models}

\begin{enumerate}
    \item An encoder neural network reads and encodes a source sentence into a fixed-length vector.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item A decoder then outputs a translation from the encoded vector.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item The whole encoderâ€“decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}





\subsection{RNN Encoder-Decoder}
\begin{enumerate}
    \item encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item Encoder:
    \begin{enumerate}
        \item reads the input sentence: sequence of vectors $x = (x_1, \cdots, x_{T_x})$
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item $h_t = f(x_t, h_{t-1}) \in \mbbR^n$ : hidden state at time $t$
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item $c = q(\dCurlyBrac{h_1, \cdots, h_{T_x}})$ :  vector generated from the sequence of the hidden states
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item $f$ and $q$ are some nonlinear functions
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \end{enumerate}

    \item Decoder:
    \begin{enumerate}
        \item trained to predict the next word $y_{t^\prime}$ given the context vector $c$ and all the previously predicted words $\dCurlyBrac{y_1, \cdots , y_{t^\prime-1}}$.
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item In other words, the decoder defines a probability over the translation $y$ by decomposing the joint probability into the ordered conditionals:\\
        $
            P(y)
            = \dprod_{t=1}^T P(y_t | \dCurlyBrac{y_1, \cdots, y_{t-1}}, c)
        $
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
        \\
        where $y = (y_1, \cdots, y_{T_y})$
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
        \\
        For RNN:
        $
            P(y_t | \dCurlyBrac{y_1, \cdots, y_{t-1}}, c)
            = g(y_{t-1}, s_t, c)
        $
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item $g$ is a nonlinear, potentially multi-layered, function that outputs the probability of $y_t$
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item $s_t$ hidden state of decoder RNN
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \end{enumerate}
\end{enumerate}







\subsubsection{LSTM Encoder-Decoder}
\begin{enumerate}
    \item achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item LSTM is used as $f$ and $c = q(\dCurlyBrac{h_1, \cdots, h_{T_x}}) = h_{T_x}$ in encoder part of RNN Encoder-Decoder
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}










\subsection{Attention-Based Encoder-Decoder Models}

\begin{enumerate}
    \item learns to align and translate jointly
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item Each time the model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated.
    The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item it does not attempt to encode a whole input sentence into a single fixed-length vector.
    Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.
    This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}











