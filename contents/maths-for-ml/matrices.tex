\chapter{Matrices}

\begin{enumerate}
    \item With $m, n \in \mathbb{N}$ a real-valued $(m, n)$ matrix $\bm{A}$ is an $m\cdot n$-tuple of elements $a_{ij}$, $i = 1, \cdots , m$, $j = 1, \cdots , n$, which is ordered according to a rectangular scheme consisting of $m$ rows and $n$ columns:
    \\[0.2cm]
    $
        \bm{A}
        =
        \begin{bmatrix}
            a_{11} & a_{12} & \cdots & a_{1n} \\
            a_{21} & a_{22} & \cdots & a_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{m1} & a_{m2} & \cdots & a_{mn} \\
        \end{bmatrix}
        \hfill
        (\ a_{ij} \in \mbbR \ )
    $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item $\mbbR^{m\times n}$ is the set of all real-valued $(m, n)$-matrices. $\bm{A} \in \mbbR^{m\times n}$ can be equivalently represented as $\bm{a} \in \mbbR^{mn}$ by stacking all $n$ columns of the matrix into a long vector.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \vspace{0.5cm}

    \item They can be used to compactly represent systems of linear equations, but they also represent linear functions (linear mappings).
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item matrix represents a linear mapping or a collection of vectors.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}


\section{Matrix Operations}
\input{contents/maths-for-ml/matrices-operations/matrix-addition}
\input{contents/maths-for-ml/matrices-operations/matrix-multiplication}
\input{contents/maths-for-ml/matrices-operations/Multiplication-by-Scalar}
\input{contents/maths-for-ml/matrices-operations/Hadamard-product}
\input{contents/maths-for-ml/matrices-operations/Matrix-Transpose}
\input{contents/maths-for-ml/matrices-operations/Matrix-Inverse}




\section{Rank of a matrix}

\begin{enumerate}
    \item
    \begin{definition}[Rank of a matrix]
        The number of linearly independent columns of a matrix $\bm{A} \in \mbbR^{m\times n}$ equals the number of linearly independent rows and is called the rank of $\bm{A}$ and is denoted by $\text{rk}(\bm{A})$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{definition}

\end{enumerate}


\subsection{Properties of rank}

\begin{enumerate}
    \item $\text{rk}(\bm{A}) = \text{rk}(\bm{A}^\top)$, i.e., the column rank equals the row rank.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The columns of $\bm{A} \in \mbbR^{m\times n}$ span a subspace $U \subseteq \mbbR^m$ with $\dim(U) = \text{rk}(\bm{A})$.
    We call this subspace the \textbf{image or range}.
    A basis of $U$ can be found by applying Gaussian elimination to $\bm{A}$ to identify the \textbf{pivot columns}.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The rows of $\bm{A} \in  \mbbR^{m\times n}$  span a subspace $W \subseteq \mbbR^n$ with $\dim(W) = \text{rk}(\bm{A})$.
    A basis of $W$ can be found by applying Gaussian elimination to $\bm{A}^\top$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item For all $\bm{A} \in  \mbbR^{n\times n}$ it holds that $\bm{A}$ is regular (invertible) if and only if $\text{rk}(\bm{A}) = n$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item For all $\bm{A} \in  \mbbR^{m\times n}$  and all $\bm{b} \in  \mbbR^m$ it holds that the linear equation system $Ax = b$ can be solved if and only if $\text{rk}(\bm{A}) = \text{rk}(\bm{A}|\bm{b})$, where $\bm{A}|\bm{b}$ denotes the augmented system.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item For $\bm{A} \in  \mbbR^{m\times n}$  the subspace of solutions for $\bm{A}\bm{x} = \bm{0}$ possesses dimension $n - \text{rk}(\bm{A})$.
    We call this subspace the \textbf{kernel} or the \textbf{null space}.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item A matrix $\bm{A} \in  \mbbR^{m\times n}$  has \textbf{full rank} if its rank equals the largest possible rank for a matrix of the same dimensions.
    This means that the rank of a full-rank matrix is the lesser of the number of rows and columns, i.e., $\text{rk}(\bm{A}) = min(m, n)$.
    A matrix is said to be \textbf{rank deficient} if it does not have full rank.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}


\end{enumerate}



\begin{lstlisting}[
    language=Python,
    caption=Rank of a matrix - numPy
]
import numpy as np

# Define your matrix
A = np.array([
    [1, 2, 3],
    [2, 4, 6],
    [1, 0, 1]
])

# Compute the rank
rank = np.linalg.matrix_rank(A)

print("Rank of matrix A:", rank)
\end{lstlisting}









\section{Null Space and Column Space}

\begin{enumerate}
    \item Let us consider $\bm{A} \in \mbbR^{m\times n}$ and a linear mapping $\Phi : \mbbR^n \to \mbbR^m$, $\bm{x} \mapsto \bm{Ax}$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item For $\bm{A} = [\bm{a}_1, \cdots , \bm{a}_n]$, where $\bm{a}_i$ are the columns of $\bm{A}$, we obtain
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    .\hfill
    $
        Im(\Phi)
        = \dCurlyBrac{\bm{Ax} : \bm{x} \in \mbbR^ n }
        = \dCurlyBrac{\dsum ^n _{i=1} x_i \bm{a}_i : x_1, \cdots , x_n \in \mbbR}
        = span[\bm{a}_1, \cdots , \bm{a}_n] \subseteq \mbbR^m
    $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    i.e., the image is the span of the columns of $\bm{A}$, also called the \textbf{column space}.
    Therefore, the column space (image) is a subspace of $\mbbR^m$, where $m$ is the “height” of the matrix.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item $\text{rk}(\bm{A}) = \dim(\text{Im}(\Phi))$
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The kernel/null space $\ker(\Phi)$ is the general solution to the homogeneous system of linear equations $\bm{Ax} = \bm{0}$ and captures all possible linear combinations of the elements in $\mbbR^n$ that produce $\bm{0} \in \mbbR^m$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The kernel is a subspace of $\mbbR^n$ , where $n$ is the “width” of the matrix.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The kernel focuses on the relationship among the columns, and we can use it to determine whether/how we can express a column as a linear combination of other columns.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}






\section{Inhomogeneous systems of linear equations and affine subspaces}

\begin{enumerate}
    \item For $\bm{A} \in \mbbR^{m\times n}$ and $\bm{x} \in \mbbR^m$, the solution of the system of linear equations $\bm{A} \bm{\lambda}  = \bm{x}$ is either the empty set or an affine subspace of $\mbbR^n$ of dimension $n - \text{rk}(\bm{A})$.
    In particular, the solution of the linear equation $\lambda _1 \bm{b}_1 + \cdots + \lambda _n \bm{b}_n = \bm{x}$, where $(\lambda _1, \cdots , \lambda _n) \neq (0, . . . , 0)$, is a hyperplane in $\mbbR^n$ .
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item In $\mbbR^n$ , every $k$-dimensional affine subspace is the solution of an inhomogeneous system of linear equations $\bm{Ax} = \bm{b}$, where $\bm{A} \in \mbbR^{m\times n}$ , $\bm{b} \in \mbbR^m$ and $\text{rk}(\bm{A}) = n - k$.
    For homogeneous equation systems $\bm{Ax} = \bm{0}$ the solution was a vector subspace, which we can also think of as a special affine space with support point $\bm{x}_0 = \bm{0}$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}

























\section{Types of matrices}
\input{contents/maths-for-ml/matrices-types/Square-Matrix}
\input{contents/maths-for-ml/matrices-types/Identity-Matrix}
\input{contents/maths-for-ml/matrices-types/regular_invertible_non-singular}
\input{contents/maths-for-ml/matrices-types/singular_non-invertible}
\input{contents/maths-for-ml/matrices-types/Symmetric-Matrix}
\input{contents/maths-for-ml/matrices-types/Symmetric-Positive-Definite-Matrix}
\input{contents/maths-for-ml/matrices-types/equivalent-matrices}
\input{contents/maths-for-ml/matrices-types/similar-matrices}
\input{contents/maths-for-ml/matrices-types/orthogonal-matrix}
\input{contents/maths-for-ml/matrices-types/triangular-matrix}
\input{contents/maths-for-ml/matrices-types/diagonal-matrix}





\section{Norms of a Matrix}

\begin{enumerate}
    \item \begin{definition}[Spectral Norm of a Matrix]
        For $\bm{x} \in \mbbR^n\backslash \dCurlyBrac{0}$, the spectral norm of a matrix $\bm{A} \in \mbbR^{m\times n}$ is defined as
        $
            \dnorm{\bm{A}}_2
            := \displaystyle\max_{\bm{x}} \dfrac{\dnorm{\bm{Ax}}_2}{\dnorm{\bm{x}}_2}
        $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{definition}
\end{enumerate}





\section{Describing/summarizing Matrix}
\input{contents/maths-for-ml/matrices-summary/determinant}
\input{contents/maths-for-ml/matrices-summary/trace}
\input{contents/maths-for-ml/matrices-summary/Characteristic-Polynomial}
\input{contents/maths-for-ml/matrices-summary/eigenvalue-eigenvector}



\section{Matrix Decomposition/Factorization}
\input{contents/maths-for-ml/matrices-decompositions/Cholesky-Decomposition}
\input{contents/maths-for-ml/matrices-decompositions/Eigen-decomposition}
\input{contents/maths-for-ml/matrices-decompositions/Singular-Value-Decomposition}


\subsection{Eigenvalue Decomposition ($A = P DP ^{-1}$) vs. Singular Value Decomposition ($A = U \Sigma V^\top$)}

\begin{enumerate}
    \item The SVD always exists for any matrix $\mbbR^{m\times n}$ .
    The eigendecomposition is only defined for square matrices $\mbbR^{n\times n}$ and only exists if we can find a basis of eigenvectors of $\mbbR^n$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The vectors in the eigendecomposition matrix $\bm{P}$ are not necessarily orthogonal, i.e., the change of basis is not a simple rotation and scaling.
    On the other hand, the vectors in the matrices $\bm{U}$ and $\bm{V}$ in the SVD are orthonormal, so they do represent rotations.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item Both the eigendecomposition and the SVD are compositions of three linear mappings:
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \begin{enumerate}
        \item Change of basis in the domain
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item Independent scaling of each new basis vector and mapping from domain to codomain
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item Change of basis in the codomain
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}
    A key difference between the eigendecomposition and the SVD is that in the SVD, domain and codomain can be vector spaces of different dimensions.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item In the SVD, the left- and right-singular vector matrices $\bm{U}$ and $\bm{V}$ are generally not inverse of each other (they perform basis changes in different vector spaces).
    In the eigendecomposition, the basis change matrices $\bm{P}$ and $\bm{P}^{ -1}$ are inverses of each other.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item In the SVD, the entries in the diagonal matrix $\bm{\Sigma}$ are all real and nonnegative, which is not generally true for the diagonal matrix in the eigendecomposition.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The SVD and the eigendecomposition are closely related through their projections:
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \begin{enumerate}
        \item The left-singular vectors of $\bm{A}$ are eigenvectors of $\bm{AA}^\top$
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The right-singular vectors of $\bm{A}$ are eigenvectors of $\bm{A}^\top \bm{A}$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The nonzero singular values of $\bm{A}$ are the square roots of the nonzero eigenvalues of both $\bm{AA}^\top$ and $\bm{A} ^\top \bm{A}$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}

    \item For symmetric matrices $\bm{A} \in \mbbR^{n\times n}$ , the eigenvalue decomposition and the SVD are one and the same, which follows from the spectral theorem.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}






\section{Matrix Approximation}

\begin{enumerate}
    \item Instead of doing the full SVD factorization, SVD allows us to represent a matrix $\bm{A}$ as a sum of simpler (low-rank) matrices $\bm{A}_i$, which lends itself to a matrix approximation scheme that is cheaper to compute than the full SVD.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item We construct a rank-1 matrix
    $
        \bm{A}_i
        := \bm{u}_i \bm{v}_i^\top
        \in \mbbR^{m\times n}
    $
    which is formed by the outer product of the $i$-th orthogonal column vector of $\bm{U}$ and $\bm{V}$ .
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item A matrix $\bm{A} \in \mbbR^{m\times n}$ of rank $r$ can be written as a sum of rank-1 matrices $\bm{A}_i$ so that
    $
        \bm{A}
        = \dsum_{i=1}^r \sigma_i \bm{u}_i \bm{v}_i^\top
        = \dsum_{i=1}^r \sigma_i \bm{A}_i
    $
    where the outer-product matrices $\bm{A}_i$ are weighted by the $i$-th singular value $\sigma_i$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item If the sum does not run over all matrices $\bm{A}_i$, $i = 1, \cdots , r$, but only up to an intermediate value $k < r$, we obtain a rank-$k$ approximation
    $
        \hat{\bm{A}}(k)
        = \dsum_{i=1}^k \sigma_i \bm{u}_i \bm{v}_i^\top
        = \dsum_{i=1}^k \sigma_i \bm{A}_i
    $
    of $\bm{A}$ with $rk(\hat{\bm{A}}(k)) = k$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item \begin{theorem}[Eckart-Young Theorem]
        Consider a matrix $\bm{A} \in \mbbR^{m\times n}$ of rank $r$ and let $\bm{B} \in \mbbR^{m\times n}$ be a matrix of rank $k$.
        For any $k \leq r$ with $\hat{\bm{A}}(k) = \dsum^k_{i=1} \sigma_i \bm{u}_i \bm{v}_i^\top$ it holds that
        ${
            \displaystyle
            \hat{\bm{A}}(k)
            = {\arg\max}_{\text{rk}(\bm{B})=k} \dnorm{\bm{A} - \bm{B}}_2
        }$,
        \hspace{0.5cm}
        ${
            \displaystyle
            \dnorm{\bm{A} - \hat{\bm{A}}(k)}_2 = \sigma_{k+1}.
        }$
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{theorem}
    \begin{enumerate}
        \item The Eckart-Young theorem states explicitly how much error we introduce by approximating $\bm{A}$ using a rank-$k$ approximation.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item We can interpret the rank-$k$ approximation obtained with the SVD as a projection of the full-rank matrix $\bm{A}$ onto a lower-dimensional space of rank-at-most-$k$ matrices.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The SVD minimizes the error (with respect to the spectral norm) between $\bm{A}$ and any rank-$k$ approximation.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The Eckart-Young theorem implies that we can use SVD to reduce a rank-$r$ matrix $\bm{A}$ to a rank-$k$ matrix $\hat{\bm{A}}$ in a principled, optimal (in the spectral norm sense) manner.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}

    \item the difference between $\bm{A} - \hat{\bm{A}}(k)$ is a matrix containing the sum of the remaining rank-$1$ matrices
    $
        \bm{A} - \hat{\bm{A}}(k)
        = \dsum_{i=k+1}^r \sigma_i \bm{u}_i \bm{v}_i^\top
    $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item If we assume that there is another matrix $\bm{B}$ with $\text{rk}(\bm{B}) \leq k$, such that
    $
        \dnorm{\bm{A} - \bm{B}}_2 \leq \dnorm{\bm{A} - \hat{\bm{A}}(k)}_2
    $,
    then there exists an at least $(n - k)$-dimensional null space $Z \subseteq \mbbR^n$, such that $\bm{x} \in \mathbb{Z}$ implies that $\bm{Bx} = \bm{0}$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    Then it follows that
    $
        \dnorm{\bm{Ax}}_2 = \dnorm{(\bm{A} - \bm{B})\bm{x}}_2
    $
    and by using a version of the Cauchy-Schwartz inequality that encompasses norms of matrices, we obtain
    $
        \dnorm{\bm{Ax}}_2
        \leq \dnorm{\bm{A} - \bm{B}}_2 \dnorm{\bm{x}}_2
        < \sigma_{k+2} \dnorm{\bm{x}}_2
    $.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    However, there exists a ($k + 1$)-dimensional subspace where $\dnorm{\bm{Ax}}_2 \geq \sigma_{k+1} \dnorm{\bm{x}}_2$, which is spanned by the right-singular vectors $\bm{v}_j$ , $j \leq k + 1$ of $\bm{A}$.
    \\
    Adding up dimensions of these two spaces yields a number greater than $n$, as there must be a nonzero vector in both spaces. This is a contradiction of the rank-nullity theorem
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \vspace{0.5cm}
    \\
    \textbf{Explaining the contradiction part}:
    \hfill \cite{common/online/chatgpt}
    \begin{enumerate}
        \item $\text{rank}(\bm{B})+\text{nullity}(\bm{B})=n$

        \item So if $\text{rank}(\bm{B})< k$, then nullity $>n-k$.

        \item Consider:
        \begin{enumerate}
            \item The subspace where $\bm{Bx}=\bm{0}$: null space of dimension $>n-k$
            \item The subspace spanned by $\bm{v}_{k+1},\cdots,\bm{v}_r$: has dimension $\geq n-k$
        \end{enumerate}

        \item If both have more than $n-k$ dimensions, their intersection must be nonzero (they must share a direction). That means:
        \begin{enumerate}
            \item There exists a nonzero vector xx that’s in both: in the null space of BB and in the span of $\bm{v}_{k+1},\cdots,\bm{v}_r$

            \item For that $\bm{x}$, we must have: $\dnorm{\bm{Ax}}_2 \geq \sigma_{k+1}$.
            But this contradicts the earlier assumption that $\dnorm{\bm{Ax}}_2 < \sigma_{k+1}$.
        \end{enumerate}

        \item Thus, assuming that $\dnorm{\bm{A}-\bm{B}}_2<\sigma_{k+1}$ leads to a contradiction with rank-nullity.
        Therefore: $\dnorm{\bm{A}-\bm{B}}_2 \geq \sigma_{k+1}$ which confirms the theorem.

        \item \textbf{in Simple Terms}:
        \begin{enumerate}
            \item You’re trying to approximate a matrix using fewer components (rank $k$).

            \item The best you can do (smallest error) is using the top $k$ singular values from SVD.

            \item If you try to do better, you run into a contradiction with basic linear algebra — namely, the rank-nullity theorem.

            \item So, SVD gives the best possible low-rank approximation, and the error is exactly the $(k+1)$-th singular value.
        \end{enumerate}
    \end{enumerate}

    \item We can interpret the approximation of $\bm{A}$ by a rank-$k$ matrix as a form of \textbf{lossy compression}.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}














