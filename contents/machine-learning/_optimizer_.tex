\chapter{ \emoji{star2} Optimizer}



\section{Regularization}

\begin{enumerate}
    \item Regularization is an important technique in machine learning that helps to improve model accuracy by preventing overfitting which happens when a model learns the training data too well including noise and outliers and perform poor on new data. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item By adding a penalty for complexity it helps simpler models to perform better on new data.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}




\subsection{Early Stopping}

\begin{enumerate}
    \item Stops training when validation performance deteriorates, preventing overfitting by halting before the model memorizes training data.
    \hfill \cite{wiki/Regularization_mathematics}
\end{enumerate}





\subsection{Dropout}

\begin{enumerate}
    \item Dropout technique repeatedly ignores random subsets of neurons during training, which simulates the training of multiple neural network architectures at once to improve generalization.
    \hfill \cite{wiki/Regularization_mathematics}
\end{enumerate}





\subsection{Lasso/ L1 Regression}

\begin{enumerate}
    \item Regularization Term = $ \dsum \dabs{w_i}$
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}

    \item A regression model which uses the L1 Regularization technique is called LASSO (Least Absolute Shrinkage and Selection Operator) regression. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function(L). 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less important ones.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}




\subsection{Ridge/ L2 Regression}

\begin{enumerate}
    \item Regularization Term = $ \dsum w_i^2$
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item A regression model that uses the L2 regularization technique is called Ridge regression. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item It adds the squared magnitude of the coefficient as a penalty term to the loss function(L). 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item It handles multicollinearity by shrinking the coefficients of correlated features instead of eliminating them.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}




\subsection{Elastic Net/ L1-L2 Regression}

\begin{enumerate}
    \item Regularization Term = $ (1-\alpha) \dsum \dabs{w_i} + \alpha\dsum w_i^2$
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    \\
    $0 \leq \alpha \leq 1$: Mixing parameter, $\alpha=0$: Lasso, $\alpha=1$: Ridge

    \item Elastic Net Regression is a combination of both L1 as well as L2 regularization. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item That shows that we add the absolute norm of the weights as well as the squared measure of the weights. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}


























