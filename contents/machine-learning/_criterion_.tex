\chapter{ \emojistar Loss Function (Criterion)}

{\centering\fontsize{22}{22}\selectfont\bfseries Regression related Loss \par}
\addcontentsline{toc}{section}{\textbf{Regression related Loss}}
\vspace{0.5cm}

\section{Sum of Errors (SE)}

\begin{enumerate}
    \item[] 
    $
        L = \dsum_{i=1}^n (\hat{y}_i - y_i)
    $
    \hfill \cite{medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-a-regression-model-27d2bbda9c93}
\end{enumerate}

\section{Sum of Absolute Errors (SAE)}

\begin{enumerate}
    \item[] 
    $
        L = \dsum_{i=1}^n (\dabs{\hat{y}_i - y_i})
    $
    \hfill \cite{medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-a-regression-model-27d2bbda9c93}
\end{enumerate}

\section{Sum of Squared Errors (SSE)}

\begin{enumerate}
    \item[] 
    $
        L = \dsum_{i=1}^n (\hat{y}_i - y_i)^2
    $
    \hfill \cite{medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-a-regression-model-27d2bbda9c93}
\end{enumerate}

\section{Mean Absolute Error (MAE) / L1 Loss}

\begin{enumerate}
    \item[] 
    $
        L = \dfrac{1}{n} \dsum_{i=1}^n (\dabs{\hat{y}_i - y_i})
    $
    \hfill \cite{medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-a-regression-model-27d2bbda9c93}
\end{enumerate}

\section{Mean Square Error (MSE) / L2 Loss}

\begin{enumerate}
    \item[] 
    $
        L = \dfrac{1}{n} \dsum_{i=1}^n (\hat{y}_i - y_i)^2
    $
    \hfill \cite{medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-a-regression-model-27d2bbda9c93}
\end{enumerate}

\section{Root Mean Square Error (RMSE) Loss}

\begin{enumerate}
    \item[] 
    $
        L = \sqrt{\dfrac{1}{n} \dsum_{i=1}^n (\hat{y}_i - y_i)^2}
    $
    \hfill \cite{medium.com/analytics-vidhya/common-loss-functions-in-machine-learning-for-a-regression-model-27d2bbda9c93}
\end{enumerate}

\section{Huber Loss / Smooth Mean Absolute Error}


\section{Mean Bias Error (MBE)}



\clearpage
{\centering\fontsize{22}{22}\selectfont\bfseries Classification related Loss \par}
\addcontentsline{toc}{section}{\textbf{Classification related Loss}}
\vspace{0.5cm}


\section{Binary Cross-Entropy Loss / Log Loss}

\begin{enumerate}
    \item[]
    $
        L = -y * log(\hat{y}) - (1 - y)  \log(1 - \hat{y})
    $
    \hfill \cite{datacamp/tutorial/loss-function-in-machine-learning}
\end{enumerate}

\section{Categorical Cross-Entropy Loss}

\section{Hinge Loss}

\begin{enumerate}
    \item 
\end{enumerate}


\section{Log Loss}




\clearpage
{\centering\fontsize{22}{22}\selectfont\bfseries Regularization \par}
\addcontentsline{toc}{section}{\textbf{Regularization}}
\vspace{0.5cm}


\begin{enumerate}
    \item Regularization is an important technique in machine learning that helps to improve model accuracy by preventing overfitting which happens when a model learns the training data too well including noise and outliers and perform poor on new data. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item By adding a penalty for complexity it helps simpler models to perform better on new data.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}




\section{Early Stopping}

\begin{enumerate}
    \item Stops training when validation performance deteriorates, preventing overfitting by halting before the model memorizes training data.
    \hfill \cite{wiki/Regularization_mathematics}
\end{enumerate}





\section{Dropout}

\begin{enumerate}
    \item Dropout technique repeatedly ignores random subsets of neurons during training, which simulates the training of multiple neural network architectures at once to improve generalization.
    \hfill \cite{wiki/Regularization_mathematics}
\end{enumerate}





\section{Lasso/ L1 Regression}

\begin{enumerate}
    \item Regularization Term = $ \dsum \dabs{w_i}$
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}

    \item \textbf{Regularized loss function} or \textbf{Objective function}:
    $
        L^\prime (\bm{w}) = L (\bm{w}) + \lambda \dsum \dabs{w_i}
    $
    \\
    $\lambda$: weight decay coefficient (hyperparameter)

    \item A regression model which uses the L1 Regularization technique is called LASSO (Least Absolute Shrinkage and Selection Operator) regression. 
    Techniques such as this are known in the statistics literature as \textbf{shrinkage} methods because they reduce the value of the coefficients. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function(L). 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less important ones.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}




\section{Ridge/ L2 Regression}

\begin{enumerate}
    \item Regularization Term = $ \dsum w_i^2 = \bm{x}^\top\bm{x}$
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}

    \item \textbf{Regularized loss function} or \textbf{Objective function}:
    $
        L^\prime (\bm{w}) = L (\bm{w}) + \lambda \bm{x}^\top\bm{x}
    $
    \\
    $\lambda$: weight decay coefficient (hyperparameter)
    
    \item A regression model that uses the L2 regularization technique is called Ridge regression. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item It adds the squared magnitude of the coefficient as a penalty term to the loss function(L). 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item It handles multicollinearity by shrinking the coefficients of correlated features instead of eliminating them.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}




\section{Elastic Net/ L1-L2 Regression}

\begin{enumerate}
    \item Regularization Term = $ (1-\alpha) \dsum \dabs{w_i} + \alpha\dsum w_i^2$
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    \\
    $0 \leq \alpha \leq 1$: Mixing parameter, $\alpha=0$: Lasso, $\alpha=1$: Ridge

    \item Elastic Net Regression is a combination of both L1 as well as L2 regularization. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item That shows that we add the absolute norm of the weights as well as the squared measure of the weights. 
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
    
    \item With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization.
    \hfill \cite{geeksforgeeks/machine-learning/regularization-in-machine-learning}
\end{enumerate}
























