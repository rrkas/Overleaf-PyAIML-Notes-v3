\chapter{Machine Learning: Introduction}

\begin{enumerate}
    \item The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item A computer program is said to \textbf{learn} from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \\
    \textbf{Example}: A checkers learning problem
    \begin{enumerate}
        \item Task $T$: playing checkers
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
        
        \item Performance measure $P$: percent of games won against opponents
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
        
        \item Training experience $E$: playing practice games against itself 
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \end{enumerate}
    
    \item \textbf{learning}: class of programs that improve through experience, not other ways like Database updates, etc
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item 
\end{enumerate}



\section{Designing A Learning System}

\subsection{Choosing the Training Experience \cite{ml/book/Machine-Learning/Tom-M-Mitchell}}

\begin{enumerate}
    \item The type of training experience available can have a \textbf{significant impact} on success or failure of the learner.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item One key attribute is whether the training experience provides direct or indirect feedback regarding the choices made by the performance system.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \begin{enumerate}
        \item For example, in learning to play checkers, the system might learn from \textbf{direct} training examples consisting of individual checkers board states and the correct move for each.
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

        \item it might have available only \textbf{indirect} information consisting of the move sequences and final outcomes of various games played. In this later case, information about the correctness of specific moves early in the game must be inferred indirectly from the fact that the game was eventually won or lost.
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \end{enumerate}

    \item learning from direct training feedback is typically easier than learning from indirect feedback.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item \textbf{credit assignment}: determining the degree to which each move in the sequence deserves credit or blame for the final outcome. Credit assignment can be a particularly difficult problem because the game can be lost even when early moves are optimal, if these are followed later by poor moves.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item A second important attribute of the training experience is the \textit{degree to which the learner controls the sequence of training examples}. 
    \begin{enumerate}
        \item For example, the learner might \textbf{rely on the teacher} to select informative board states and to provide the correct move for each. 
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

        \item Alternatively, the learner might \textbf{itself} propose board states that it finds particularly confusing and ask the teacher for the correct move.
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

        \item Or the learner may have complete control over \textbf{both} the board states and (indirect) training classifications, as it does when it learns by playing against itself with no teacher present.
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
        \\
        learner may choose between experimenting with novel board states that it has not yet considered, or honing its skill by playing minor variations of lines of play it currently finds most promising.
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \end{enumerate}

    \item A third important attribute of the training experience is how well it represents the distribution of examples over which the final system performance $P$ must be measured.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \begin{enumerate}
        \item learning is most reliable when the training examples follow a distribution similar to that of future test examples.
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

        \item 
        \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \end{enumerate}
\end{enumerate}




\subsection{Choosing the Target Function ( $V$ ) }

\begin{enumerate}
    \item determine exactly what type of knowledge will be learned and how this will be used by the performance program.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item Let us begin with a checkers-playing program that can generate the \textbf{legal moves} from any board state. The program needs only to learn how to choose the \textbf{best move} from among these legal moves.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item function $ChooseMove$ chooses the best move for any given board state
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item it is useful to reduce the problem of improving performance $P$ at task $T$ to the problem of learning some particular target function.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item $ChooseMove : B \to M$ indicates that this function accepts as input any board from the set of legal board states $B$ and produces as output some move from the set of legal moves $M$.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item let \textbf{evaluation function}: $V$
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item notation $V : B \to \mathbb{R}$ denote that $V$ maps any legal board state from the set $B$ to some real value (we use $\mathbb{R}$ to denote the set of real numbers). We intend for this target function $V$ to assign higher scores to better board states. If the system can successfully learn such a target function $V$, then it can easily use it to select the best move from any current board position.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item This can be accomplished by generating the successor board state produced by every legal move, then using $V$ to choose the best successor state and therefore the best legal move. 
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item \textbf{operational description of $V$}: description that can be used by the checkers-playing program to evaluate states and select moves within realistic time bounds
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \\
    \textbf{nonoperational definition}: definition is not efficiently computable
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item It may be very difficult in general to learn such an operational form of $V$ perfectly. In fact, we often expect learning algorithms to acquire only some approximation to the target function, and for this reason the process of learning the target function is often called \textbf{function approximation} ( $\hat{V}$ ).
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item In general, this choice of representation ( $\hat{V}$ ) involves a crucial tradeoff.
    \\
    On one hand, we wish to pick a very expressive representation to allow representing as close an approximation as possible to the ideal target function $V$.
    \\
    On the other hand, the more expressive the representation, the more training data the program will require in order to choose among the alternative hypotheses it can represent.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}



    \item[] \textbf{example}: Let:
    \\
    $x_1$: the number of black pieces on the board 
    \\
    $x_2$: the number of red pieces on the board 
    \\
    $x_3$: the number of black kings on the board 
    \\
    $x_4$: the number of red kings on the board 
    \\
    $x_5$: the number of black pieces threatened by red (i.e., which can be captured on red's next turn) 
    \\
    $x_6$: the number of red pieces threatened by black 
    \vspace{0.3cm} \noindent
    \\
    Then: 
    \\
    Partial design of a checkers learning program: 
    \\ \\
    .\hfill \textbf{(specification of the learning task)} \hfill.
    \\ \\
    Task $T$: playing checkers 
    \\
    Performance measure $P$: percent of games won in the world tournament 
    \\
    Training experience $E$: games played against itself 
    \\ \\
    .\hfill \textbf{(design choices for the implementation of the learning program)} \hfill.
    \\ \\
    Target function: $V: Board \to \mathbb{R}$ 
    \\
    Target function representation: $\hat{V}(b) = w_0 + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + w_6x_6$ 
    \\
    where $w_0$ through $w_6$ are numerical coefficients, or weights, to be chosen by the learning algorithm. Learned values for the weights $w_1$ through $w_6$ will determine the relative importance of the various board features in determining the value of the board, whereas the weight $w_0$ will provide an additive constant to the board value. 
    
    \item each training example: $\dAngleBrac{b, V_{train}(b)}$
        \hfill (board state $b$; training value $V_{train}(b)$ for $b$)

    \item the fact that the game was eventually won or lost does not necessarily indicate that every board state along the game path was necessarily good or bad.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    \\
    For example, even if the program loses the game, it may still be the case that board states occurring early in the game should be rated very highly and that the cause of the loss was a subsequent poor move. 
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item Despite the ambiguity inherent in estimating training values for intermediate board states, one simple approach has been found to be surprisingly successful.
    This approach is to assign the training value of $V_{train}(b)$ for any intermediate board state $b$ to be $\hat{V}(Successor(b))$, where $\hat{V}$ is the learner's current approximation to $V$ and where $Successor(b)$ denotes the next board state following $b$ for which it is again the program's turn to move.
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}

    \item 
    \textbf{Rule for estimating training values}: $V_{train}(b) \gets \hat{V}(Successor(b))$
    \hfill \cite{ml/book/Machine-Learning/Tom-M-Mitchell}
    
    \item 
    $\hat{V}(Successor(b))$ = training value of $V_{train}(b)$ for any intermediate board state $b$
    \\
    .\hspace{1cm} $\hat{V}$ is the learner's current approximation to $V$
    \\
    .\hspace{1cm} $Successor(b)$ denotes the next board state following $b$
    \vspace{0.3cm} \noindent
    
\end{enumerate}

































