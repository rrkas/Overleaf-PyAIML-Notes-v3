\chapter{Transformer (2017)}

\begin{tcolorbox}
\fullcite{arxiv/1706.03762/Attention-Is-All-You-Need}
\end{tcolorbox}



\begin{enumerate}
    \item The traditional sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.
    Some models also connect the encoder and decoder through an attention mechanism.
    \hfill \cite{arxiv/1706.03762/Attention-Is-All-You-Need}

    \item The Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 
    \hfill \cite{arxiv/1706.03762/Attention-Is-All-You-Need}

    \item 
\end{enumerate}




