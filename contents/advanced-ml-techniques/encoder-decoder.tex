\chapter{Encoder-Decoder Architecture}

AKA Sequence-to-Sequence (seq2seq)

\begin{enumerate}
    \item An \textbf{encoder} neural network reads and encodes an input sequence into a \textbf{fixed-length vector}. 
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    
    \item A \textbf{decoder} then outputs from the encoded vector.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}


\section{Disadvantages}
\begin{enumerate}
    \item A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item This may make it difficult for the neural network to cope with long sequences, especially those that are longer than the sequences in the training corpus. 
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item  performance of a basic encoder–decoder deteriorates rapidly as the length of an input sequence increases.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}












