\chapter{Attention Mechanism (2014)}

\begin{center}
    \textbf{Source}: \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate},
    \textbf{Domain-Agnostic made using}: \cite{common/online/chatgpt}
\end{center}



\section{Intro: Encoder-decoder \& Seq2seq}

\begin{enumerate}
    \item \textbf{Neural sequence modeling} is a framework in which a single neural network is trained end-to-end to map an input sequence to an output sequence. 
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item \textbf{Traditional models} often rely on \textbf{manually designed} intermediate representations or \textbf{fixed-length embeddings} that attempt to compress all input information into one vector. 
    This can create a bottleneck, since a single fixed-size representation may not capture all the necessary details of complex inputs. 
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item To overcome this limitation, we can allow the model to \textbf{dynamically focus on specific parts} of the input that are most relevant to generating each part of the output. 
    Instead of relying on a single static encoding, the model learns to compute \textbf{soft alignments}—that is, differentiable attention weights—over the input elements for each output step. 
    This enables the system to retrieve context \textbf{adaptively} and improve performance across diverse tasks.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Empirical results show that models with such attention mechanisms often outperform fixed-representation architectures, and the learned alignments correspond intuitively to which input regions influence particular outputs.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Many modern neural models for sequence or structured data processing follow an encoder–decoder architecture.
    The encoder processes the input data (e.g., a sequence, image, or signal) and compresses it into a single fixed-length representation. The decoder then uses this representation to generate or predict an output sequence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
    \begin{enumerate}
        \item This encoder–decoder system is trained jointly so that the generated outputs are as accurate as possible given the inputs. 
        \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
        
        \item A potential \textbf{limitation} of this setup is that it requires the entire input to be compressed into a single fixed-size vector. 
        This compression can lead to \textbf{information loss}, particularly for \textbf{long or complex inputs}. 
        As a result, performance tends to degrade when the input size or complexity increases, because the fixed-length representation cannot retain all relevant details.
        \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
    \end{enumerate}

    \item To address the limitations of fixed-length representations, we extend the encoder–decoder framework by allowing the model to \textbf{jointly learn to focus and predict}.
    At each output step, the model dynamically determines which parts of the input are most relevant for generating the current output element.
    Instead of compressing the entire input into a single vector, the model performs a soft search over all input positions to identify where important information is concentrated.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Using these relevance scores (attention weights), the model computes a \textbf{context vector} as a weighted combination of the input representations.
    This context vector summarizes the most pertinent information for the current prediction and is used—along with information from previous outputs—to generate the next element in the output sequence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item The key advantage of this mechanism is that it \textbf{eliminates the fixed-size bottleneck} of traditional encoder–decoder models.
    By representing the input as a sequence (or set) of feature vectors and selecting among them adaptively, the model can handle inputs of arbitrary length and complexity without being forced to compress all information into one vector.
    This flexibility allows it to retain richer details and cope more effectively with long or information-dense inputs.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Empirical studies show that models trained to learn attention jointly with prediction achieve \textbf{significant performance improvements} compared to basic encoder–decoder models.
    Moreover, the learned attention weights often provide meaningful insight into how the model associates different parts of the input with particular outputs—demonstrating that the learned “alignments” reflect human-interpretable relevance patterns.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
\end{enumerate}




\section{Probabilistic Approach}

\begin{enumerate}
    \item From a probabilistic perspective, many learning tasks can be viewed as finding an output sequence $y$ that maximizes the \textbf{conditional probability} $P(y|x)$, where $x$ represents the input data.
    In practice, we train a parameterized model to approximate this conditional distribution using paired input–output data. 
    Once the conditional model is learned, predictions can be generated by selecting the output sequence that maximizes $P(y|x)$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Recent advances have introduced \textbf{neural network–based approaches} to directly model this conditional distribution. 
    These approaches typically follow an \textbf{encoder–decoder design}: the encoder transforms the input $x$ into a learned internal representation, and the decoder generates the output $y$ conditioned on this representation.
    Recurrent neural networks (RNNs) or other sequential architectures (such as transformers or temporal CNNs) can be employed to handle variable-length inputs and outputs, encoding an input of arbitrary length into a fixed-length vector and then decoding it into a corresponding output sequence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Although relatively new when first introduced, these neural encoder–decoder models demonstrated strong empirical performance on structured prediction tasks.
    For instance, recurrent architectures with memory mechanisms such as LSTMs were shown to achieve or surpass previous state-of-the-art results by capturing long-range dependencies between elements of the input and output.
    Further improvements were obtained by integrating neural sequence models with existing systems or by re-ranking candidate outputs using learned probabilistic scores.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
\end{enumerate}


\subsection{Encoder}

\begin{enumerate}
    \item the encoder processes the input — represented as a sequence of vectors: $\bm{x}=(x_1,\cdots,x_{T_x})$ — and transforms it into an internal representation $c$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item A common implementation uses a recurrent neural network (RNN), where each hidden state $h_t$ is updated based on the current input and the previous hidden state: 
    \colorbox{yellow}{$h_t=f(x_t,h_{t-1})$}
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item The sequence of hidden states $\dCurlyBrac{h_1,\cdots,h_{T_x}}$ captures information about the entire input sequence.
    The final representation (or context vector) $c$ is then computed as a function of these hidden states: \colorbox{yellow}{$c=q(\dCurlyBrac{h_1,\cdots,h_{T_x}})$} 
    where $f$ and $q$ are nonlinear functions, such as RNN or LSTM units and pooling operations (e.g., taking the last hidden state or an aggregation of all).
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item In practice, $f$ defines how information is updated at each step (e.g., through gated mechanisms like LSTMs or GRUs), and $q$ defines how the entire sequence is summarized into a single vector representation.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
\end{enumerate}











