\chapter{Attention Mechanism (2014)}

\begin{center}
    \textbf{Source}: \cite{adv-ml-tech/paper/arxiv.org/1409.0473}, 
    \textbf{General Context Extracted Using}: \cite{common/online/chatgpt}
\end{center}


\begin{enumerate}
    \item allows the model to dynamically and softly focus on different parts of the input when generating each part of the output, without needing to explicitly isolate those parts in advance. 
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473, common/online/chatgpt}

    \item This approach leads to improved results and produces attention patterns that align with human intuition about which input elements are most relevant at each step.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473, common/online/chatgpt}

    \item an extension to the sequence model that learns to align relevant parts of the input while generating the output. 
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473, common/online/chatgpt}

    \item At each step of output generation, the model softly identifies positions in the input sequence that contain the most relevant information.
    It then uses this focused context, along with all previously generated outputs, to predict the next output element.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473, common/online/chatgpt}

    \item it avoids compressing the entire input into a single fixed-length vector. Instead, it encodes the input sequence into a sequence of vectors and adaptively selects a subset of these vectors during output generation. 
    This removes the burden of forcing all input information—regardless of length—into one compact representation, enabling the model to handle longer inputs more effectively.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473, common/online/chatgpt}

    
\end{enumerate}


\section{Architecture}


\begin{enumerate}
    \item input: $x = (x_1, \cdots, x_{T_x})$
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}

\subsection{Encoder: a Bi-directional RNN}

\begin{enumerate}
    \item hidden states/ \textbf{annotations}: $(h_1, \cdots, h_{T_x})$
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \begin{enumerate}
        \item  Each annotation $h_i$ contains information about the whole input sequence with a strong focus on the parts surrounding the $i$-th word of the input sequence.
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \end{enumerate}

    
    \item Forward RNN ($\overset{\rightarrow}{f}$) reads the input sequence as it is ordered (from $x_1$ to $x_{T_x}$ ) and calculates a sequence of forward hidden states ( $\overset{\rightarrow}{h}_1, \cdots , \overset{\rightarrow}{h}_{T_x}$ ).
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item backward RNN ($\overset{\leftarrow}{f}$) reads the sequence in the reverse order (from $x_{T_x}$ to $x_1$), resulting in a sequence of backward hidden states ( $\overset{\leftarrow}{h}_1, \cdots , \overset{\leftarrow}{h}_{T_x}$ ).
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item  annotation for each word $x_j$ by concatenating the forward hidden state $\overset{\rightarrow}{h} _j$ and the backward one $\overset{\leftarrow}{h} _j$
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \\
    .\hfill
    $
        h_j = \dSquareBrac{
            \overset{\rightarrow}{h} _j^\top ; \ 
            \overset{\leftarrow}{h} _j^\top
        }
    $
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \\
    In this way, the annotation $h_j$ contains the summaries of both the preceding words and the following words.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item Due to the tendency of RNNs to better represent recent inputs, the annotation $h_j$ will be focused on the words around $x_j$.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
\end{enumerate}

\subsection{Decoder}

\begin{enumerate}
    \item emulates searching through source sequence during decoding
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item decoder hidden state: $s_i = f(s_{i-1}, y_{i-1}, c_i)$
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item conditional probability: $p(y_i | y_1, \cdots , y_{i-1}, x) = g(y_{i-1}, s_i, c_i)$
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}


    \item context vector: $c_i$ depends on annotations:
    $
        c_i
        = \dsum_{j=1}^{T_x} \alpha_{ij} h_j
    $
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \begin{enumerate}
        \item aka expected annotation
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
        
        \item $\alpha_{ij}$:  probability that the target word $y_i$ is aligned to, or translated from, a source word $x_j$
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

        \item $i$-th context vector $c_i$ is the expected annotation over all the annotations with probabilities $\alpha_{ij}$
        \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    \end{enumerate}



    \item weight $\alpha_{ij}$ of each annotation $h_j$:
    $
        \alpha_{ij}
        = \dfrac{exp(e_{ij})}{\tsum_{k=1}^T exp(e_{ik})}
    $
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item alignment model/ energy: $e_{ij} = a(s_{i-1}, h_j)$\\
    scores how well the inputs around position $j$ and the output at position $i$ match.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}
    
    \item  We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item the alignment is not considered to be a latent variable. 
    Instead, the alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through. 
    This gradient can be used to train the alignment model as well as the whole translation model jointly.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item The probability $\alpha_{ij}$, or its associated energy $e_{ij}$, reflects the importance of the annotation $h_j$ with respect to the previous hidden state $s_{i-1}$ in deciding the next state $s_i$ and generating $y_i$.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    \item This implements a mechanism of attention in the decoder. 
    The decoder decides parts of the source sentence to pay attention to. 
    By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed-length vector.
    \hfill \cite{adv-ml-tech/paper/arxiv.org/1409.0473}

    
\end{enumerate}

























