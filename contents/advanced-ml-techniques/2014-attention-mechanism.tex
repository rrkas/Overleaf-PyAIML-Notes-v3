\chapter{Attention Mechanism (2014)}

\begin{tcolorbox}
\fullcite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
\end{tcolorbox}


\section{Intro: Encoder-decoder \& Seq2seq}

\begin{enumerate}
    \item \textbf{Neural sequence modeling} is a framework in which a single neural network is trained end-to-end to map an input sequence to an output sequence. 
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item \textbf{Traditional models} often rely on \textbf{manually designed} intermediate representations or \textbf{fixed-length embeddings} that attempt to compress all input information into one vector. 
    This can create a bottleneck, since a single fixed-size representation may not capture all the necessary details of complex inputs. 
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item To overcome this limitation, we can allow the model to \textbf{dynamically focus on specific parts} of the input that are most relevant to generating each part of the output. 
    Instead of relying on a single static encoding, the model learns to compute \textbf{soft alignments}—that is, differentiable attention weights—over the input elements for each output step. 
    This enables the system to retrieve context \textbf{adaptively} and improve performance across diverse tasks.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Empirical results show that models with such attention mechanisms often outperform fixed-representation architectures, and the learned alignments correspond intuitively to which input regions influence particular outputs.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Many modern neural models for sequence or structured data processing follow an encoder–decoder architecture.
    The encoder processes the input data (e.g., a sequence, image, or signal) and compresses it into a single fixed-length representation. The decoder then uses this representation to generate or predict an output sequence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
    \begin{enumerate}
        \item This encoder–decoder system is trained jointly so that the generated outputs are as accurate as possible given the inputs. 
        \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
        
        \item A potential \textbf{limitation} of this setup is that it requires the entire input to be compressed into a single fixed-size vector. 
        This compression can lead to \textbf{information loss}, particularly for \textbf{long or complex inputs}. 
        As a result, performance tends to degrade when the input size or complexity increases, because the fixed-length representation cannot retain all relevant details.
        \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
    \end{enumerate}

    \item To address the limitations of fixed-length representations, we extend the encoder–decoder framework by allowing the model to \textbf{jointly learn to focus and predict}.
    At each output step, the model dynamically determines which parts of the input are most relevant for generating the current output element.
    Instead of compressing the entire input into a single vector, the model performs a soft search over all input positions to identify where important information is concentrated.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Using these relevance scores (attention weights), the model computes a \textbf{context vector} as a weighted combination of the input representations.
    This context vector summarizes the most pertinent information for the current prediction and is used—along with information from previous outputs—to generate the next element in the output sequence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item The key advantage of this mechanism is that it \textbf{eliminates the fixed-size bottleneck} of traditional encoder–decoder models.
    By representing the input as a sequence (or set) of feature vectors and selecting among them adaptively, the model can handle inputs of arbitrary length and complexity without being forced to compress all information into one vector.
    This flexibility allows it to retain richer details and cope more effectively with long or information-dense inputs.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Empirical studies show that models trained to learn attention jointly with prediction achieve \textbf{significant performance improvements} compared to basic encoder–decoder models.
    Moreover, the learned attention weights often provide meaningful insight into how the model associates different parts of the input with particular outputs—demonstrating that the learned “alignments” reflect human-interpretable relevance patterns.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
\end{enumerate}




\section{Probabilistic Approach}

\begin{enumerate}
    \item From a probabilistic perspective, many learning tasks can be viewed as finding an output sequence $y$ that maximizes the \textbf{conditional probability} $P(y|x)$, where $x$ represents the input data.
    In practice, we train a parameterized model to approximate this conditional distribution using paired input–output data. 
    Once the conditional model is learned, predictions can be generated by selecting the output sequence that maximizes $P(y|x)$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Recent advances have introduced \textbf{neural network–based approaches} to directly model this conditional distribution. 
    These approaches typically follow an \textbf{encoder–decoder design}: the encoder transforms the input $x$ into a learned internal representation, and the decoder generates the output $y$ conditioned on this representation.
    Recurrent neural networks (RNNs) or other sequential architectures (such as transformers or temporal CNNs) can be employed to handle variable-length inputs and outputs, encoding an input of arbitrary length into a fixed-length vector and then decoding it into a corresponding output sequence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item Although relatively new when first introduced, these neural encoder–decoder models demonstrated strong empirical performance on structured prediction tasks.
    For instance, recurrent architectures with memory mechanisms such as LSTMs were shown to achieve or surpass previous state-of-the-art results by capturing long-range dependencies between elements of the input and output.
    Further improvements were obtained by integrating neural sequence models with existing systems or by re-ranking candidate outputs using learned probabilistic scores.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
\end{enumerate}


\subsection{RNN Encoder-Decoder}


\begin{figure}[H]
    \centering
    \includegraphics[
        width=\linewidth,
        height=5cm,
        keepaspectratio,
    ]{images/advanced-ml/arxiv-1409.0473v7.fig-1.model.png}
    \caption*{
        The graphical illustration of the proposed model trying to generate the $t$-th target word $y_t$ given a source sentence $(x_1, x_2, \cdots , x_T )$.
        \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
    }
\end{figure}


\subsubsection*{Encoder}

\begin{enumerate}
    \item the encoder processes the input — represented as a sequence of vectors: $\bm{x}=(x_1,\cdots,x_{T_x})$ — and transforms it into an internal representation $c$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item A common implementation uses a recurrent neural network (RNN), where each hidden state $h_t$ is updated based on the current input and the previous hidden state: 
    \colorbox{yellow}{$h_t=f(x_t,h_{t-1})$}
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}

    \item The sequence of hidden states $\dCurlyBrac{h_1,\cdots,h_{T_x}}$ captures information about the entire input sequence.
    The final representation (or context vector) $c$ is then computed as a function of these hidden states: \colorbox{yellow}{$c=q(\dCurlyBrac{h_1,\cdots,h_{T_x}})$} 
    where $h_t \in \mbbR^n$ is a hidden state at time $t$, and $c$ is a vector generated from the sequence of the hidden states. 
    $f$ and $q$ are some nonlinear functions.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate, common/online/chatgpt}
    \begin{enumerate}
        \item LSTM can be used as $f$ and $q (\dCurlyBrac{h_1, \cdots , h_T }) = h_T$
        \hfill [Sutskever et al. (2014)] \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \end{enumerate}

\end{enumerate}




\subsubsection*{Decoder}

\begin{enumerate}
    \item The decoder is often trained to predict the next word/token $y_{t^\prime}$ given the context vector $c$ and all the previously predicted words $\dCurlyBrac{y_1, \cdots , y_{t^\prime-1}}$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item In other words, the decoder defines a probability over the translation $\bm{y}$ by decomposing the joint probability into the ordered conditionals:
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    .\hfill
    $
        p(\bm{y}) = \dprod_{t=1}^T P(y_t | \dCurlyBrac{y_1, \cdots , y_{t-1}} , c)
    $
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    where $\bm{y} = (y_1, \cdots , y_{T_y})$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item With an RNN, each conditional probability is modeled as:
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    .\hfill
    $ P(y_t | \dCurlyBrac{y_1, \cdots , y_{t-1}} , c) = g(y_{t-1}, s_t, c) $
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    where $g$ is a nonlinear, potentially multi-layered, function that outputs the probability of $y_t$, and $s_t$ is the hidden state of the RNN.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item Other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
\end{enumerate}









\section{Learning (RNN Encoder-Decoder)}

\subsection{Decoder}

\begin{enumerate}
    \item we define each conditional probability as:
    $ P(y_i|y_1, \cdots , y_{i-1}, x) = g(y_{i-1}, s_i, c_i) $
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    where $s_i$ is an RNN hidden state for time $i$, computed by 
    $s_i = f (s_{i-1}, y_{i-1}, c_i)$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item unlike the existing encoder–decoder approach ($p(\bm{y}) = \dprod_{t=1}^T P(y_t | \dCurlyBrac{y_1, \cdots , y_{t-1}} , c)$), here the probability is conditioned on a distinct context vector $c_i$ for each target word $y_i$.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item The context vector $c_i$ depends on a sequence of annotations $(h_1, \cdots , h_{T_x} )$ to which an encoder maps the input sentence. 
    Each annotation $h_i$ contains information about the whole input sequence with a strong focus on the parts surrounding the $i$-th word of the input sequence. 
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item The context vector $c_i$ is, then, computed as a weighted sum of these annotations $h_i$:
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    .\hfill
    $ ci = \dsum_{j=1}^{T_x} \alpha_{ij} h_j  $
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}

    \item The weight $\alpha_{ij}$ of each annotation $h_j$ is computed by:
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    .\hfill
    $ \alpha_{ij}  =\dfrac{\exp (e_{ij} )}{\tsum_{k=1}^{T_x} \exp (e_{ik}) } $
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
    \\[0.2cm]
    where \colorbox{yellow}{$e_{ij} = a(s_{i-1}, h_j )$} is an alignment model which scores how well the inputs around position $j$ and the output at position $i$ match. 
    The score is based on the RNN hidden state $s_{i-1}$ (just before emitting $y_i$, $ P(y_i|y_1, \cdots , y_{i-1}, x) = g(y_{i-1}, s_i, c_i) $) and the $j$-th annotation $h_j$ of the input sentence.
    \hfill \cite{arxiv/1409.0473/NMT-Jointly-Learning-Align-Translate}
\end{enumerate}














\section{How Attention Mechanism Works?}

\begin{enumerate}
    \item \textbf{Input Encoding}: Input data is transformed into a format that the model can process and creating representations of the data.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

    \item \textbf{Query Generation}: A query vector is generated based on the current state or context of the model. 
    This query tells the model what it is looking for in the input data.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

    \item \textbf{Key-Value Pair Creation}: Input is splitted into key-value pairs:
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \begin{enumerate}
        \item \textbf{Keys} represents the important information required to measure the relevant data.
        \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
        
        \item \textbf{Values} hold actual data.
        \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \end{enumerate}

    \item \textbf{Similarity Computation}: Model calculates similarity between the query vector and each key. This helps find how relevant each part of the input is. 
    Various methods can be used to calculate this similarity such as dot products or cosine similarity.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \\[0.2cm]
    $
        \text{Score}(s,i) = 
        \begin{cases}
            h_s^{(1)}\cdot y_i & \text{ Dot Product} \\[0.2cm]
            (h_s^{(2)})^\top Wy_i & \text{ General} \\[0.2cm]
            v^\top \tanh\dParenBrac{W\dSquareBrac{\dfrac{h_s}{y_i}}} & \text{ Concat}
        \end{cases}
    $
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \\[0.2cm]
    \begin{enumerate}
        \item $h_s$: Encoder Source hidden state at position $s$
        \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

        \item $y_i$: Encoder Target hidden state at the position $i$
        \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

        \item $W$: Weight Matrix
        \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

        \item $v$: Weight vector
        \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \end{enumerate}
 
    \item \textbf{Attention Weights Calculation}: Similarity scores are passed through a softmax function to find attention weights. 
    These weights indicate the importance of each key-value pair.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \\[0.2cm]
    .\hfill
    $\text{Attention Weight}(\alpha(s,i))=\text{softmax}(\text{Similarity Scores}(s,i))$
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

    \item \textbf{Weighted Sum}: Attention weights are applied to the corresponding values which helps in generating a weighted sum. 
    This step adds the relevant information from the input based on their importance calculated by the attention mechanism.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \\[0.2cm]
    .\hfill
    $ c_t = \dsum^{T_s}_{i=1} \alpha(s,i)\ h_j^{(1)} $
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
    \\
    where $T_s$:  Total number of key-value pairs (source hidden states) in the encoder.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

    \item \textbf{Context Vector}: Weighted sum act as a context vector which represents attended information from the input. It captures the relevant context for the current task. 
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

    \item \textbf{Integration with the Model}: Context vector is combined with the model's current state or hidden representation which provides additional information for other steps of the model.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}

    \item \textbf{Repeat}: Steps 2 to 8 are repeated for each step of the model which allows the attention mechanism to focus on different parts of the input sequence or data.
    \hfill \cite{geeksforgeeks/artificial-intelligence/ml-attention-mechanism/}
\end{enumerate}





