\section{Bernoulli distribution ($X \sim B(p)$)}



\subsection{Distribution of the Sample Statistic ($T_n$)}

\subsubsection{Distribution of the Sample Average}

\begin{enumerate}
    \item Let $X_1 , X_2, \cdots , X _n$ are i.i.d. Bernoulli distributed, $X _i \sim \mathcal{B} ( p)$, the sum of the random variables is binomial $\text{Bin} (n, p)$ distributed.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item This implies that the distribution function of the sample average $\bar{X}$ of Bernoulli distributed random variables in $x \in [0, 1]$ is given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $
        F _{\bar{X}} (x)
        = P ( \bar{X} \leq x)
        = P (X_1 + X_2 + \cdots + X _n \leq nx)
        = \dsum ^{\dfloor{nx}} _{k=0} \dbinom{n}{k} p ^k (1 - p)^{n-k}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}


\end{enumerate}





\subsection{Maximum Likelihood Estimation (MLE)}


\begin{enumerate}
    \item Let $X_1 , X_2, \cdots , X_ n$ having an i.i.d. Bernoulli $\mathcal{B} ( p)$ distribution.
    We would like to estimate the parameter $p$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If we obtain a realization $x_1 , x_2, \cdots , x _n$ , we could ask how likely it is that we observe this set of results for given values of $p$.
    This so-called \textbf{likelihood} of the data is given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $
        L ( p)
        = p^ {x_1} (1 - p)^{1-x_1}\ p ^{x_2} (1 - p)^{1-x_2} \cdots p ^{x_n} (1 - p)^{1-x_n}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the term $p^ {x _i} (1 - p)^{1-x_i}$ is the probability that the random variable $X _i$ will attain the realization $x _i$ , i.e. $P (X _i = x _i ) = p^ {x _i} (1 - p)^{1-x_i}$ , with $x_ i \in \dCurlyBrac{0, 1}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The product $L ( p)$ represents the probability that $X_1 , X_2, \cdots , X _n$ will attain the realization $x_1 , x_2, \cdots , x _n$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item exploiting the i.i.d. assumption:
    \begin{enumerate}
        \item we assume identical distributions for each random variable $X _i$ (and we thus work with parameters $p$ as opposed to $p_i$)
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item the probability of the joint observations is given by the product over the probability of the individual observations; this is possible by virtue of the independence assumption
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item Since the likelihood is a function of the parameter $p$, we can search for a $p$ that would maximize the likelihood.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The maximum likelihood estimate $\hat{p}$ is the value that maximizes $L ( p)$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item To obtain this maximum, it is more convenient to take the logarithm of the likelihood $\ell( p)$ given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $
         \ell(p)
         = \dsum ^n _{i=1} [x _i \log ( p) + (1 - x_ i ) \log (1 - p)]
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    The logarithm of a function achieves its maximum value at the same points as the function itself, but is \textbf{easier} to work with analytically as we can replace the multiplications by sums that are easier to differentiate.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the maximum of a function can be obtained by taking the derivative and then equating it to zero and solving the equation for the variable of interest
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        \begin{aligned}
            &\dfrac{d \ell(p)}{dp}
            = \ell^\prime ( p)
            = \dsum ^n _{i=1} \dParenBrac{\dfrac{x _i}{p} - \dfrac{1 - x_ i} {1 - p}} = 0
            \Leftrightarrow (1 - p) \dsum^n _{i=1} x_ i - np + p \dsum^n _{i=1} x_ i = 0 \\
            &\Leftrightarrow np = \dsum^n _{i=1} x_ i
            \Leftrightarrow p = \dfrac{1}{n} \dsum^n _{i=1} x_ i = \bar{x}
        \end{aligned}
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    $

    \item The maximum likelihood estimate is now given by $\hat{p} = \bar{x}$.
    Since any value $\bar{x} - \varepsilon,\ \varepsilon > 0$, for $p$ in the derivative gives a positive value the solution is a maximum.
    The ML estimator is now $\hat{p} = \bar{X}$, which is the same as the MME, since the first moment of a Bernoulli random variable is $p$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}


\end{enumerate}



\subsection{Estimating the Parameter using Bayesian Methods}

\begin{enumerate}
    \item Let $\theta$ denote the probability of heads when throwing a (possibly unfair) coin, and let us treat the observations $X_1, \cdots , X_n$ from the coin as i.i.d. $\mathcal{B}(\theta)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Using $x$ to denote the vector of all $x_i$ realizations, we have:
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        \ell(\theta) 
        = P(x|\theta)
        = \dprod^n_{ i=1} \theta^{x _i} (1 - \theta)^{1-x_i}
        = \theta^{[\tsum^n _{i=1} x_i]} (1 - \theta)^{[n-\tsum^n_{i=1} x_i ]}
        = \theta^{n \bar{x} _n} (1 - \theta)^{[n(1- \bar{x}_ n )]}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Let prior be the support (i.e., $f (\theta) > 0$) for all plausible values of $\theta$.
    As in our case $\theta$ is a Bernoulli $p$ for which $0 \leq p \leq 1$ we should choose $f (\theta)$ such that it has support on $[0, 1]$. 
    It seems reasonable, in the absence of any other information, to choose a prior distribution that gives equal likelihood to all plausible values of $\theta$: the uniform distribution function on $[0, 1]$ thus seems a reasonable choice. 
    The uniform on $[0, 1]$ is however a special case of a much more flexible distribution function called the \textbf{beta distribution} function.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The $\text{Beta}(\alpha, \beta)$ distribution function is a continuous distribution function with PDF: ($\alpha > 0$ and $\beta > 0$)
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        f (\theta) = \dfrac{\Gamma (\alpha + \beta) }{\Gamma (\alpha)\ \Gamma (\beta)}\ \theta^{\alpha-1}(1 - \theta)^{\beta-1}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.1cm]
    for $0 < \theta < 1$ and zero otherwise.
    Gamma function $\Gamma (x) =\dint ^\infty _0 s^ {x-1}\ e^{-s}\ ds$.
    When we choose $\alpha = 1$ and $\beta = 1$ the resulting beta distribution function is actually a \textbf{uniform distribution} function on $[0, 1]$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  The $\text{Beta}(1,1)$ is considered a relatively \textbf{uninformative prior} for the Bernoulli $p$.
    The density with its peak at $0.5$ is given by $\alpha = \beta = 5$; as long as both parameters of the beta are equal and larger than $1$ the maximum of the density will be $0.5$. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{posterior}:
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        \begin{aligned}
            f (\theta|x) 
            &\propto P(x|\theta) f (\theta) \\
            &= \theta^{(\tsum^n _{i=1} x_i)} (1 - \theta)^{(n-\tsum^n _{i=1} x _i)} \dfrac{\Gamma (\alpha + \beta) }{\Gamma (\alpha)\Gamma (\beta)} \theta ^{\alpha-1}(1 - \theta)^{\beta-1} \\
            &\propto \theta^{(\alpha+\tsum^n _{i=1} x _i )-1}(1 - \theta)^{ (\beta+n-\tsum^n_{ i=1} x_ i )-1}
        \end{aligned}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  the posterior $f (\theta|D)$ is proportional to the density of a $\text{Beta}(\alpha^\prime, \beta^\prime)$ distribution, with
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item $\alpha^\prime = \alpha + \dsum^n _{i=1} x_ i$
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item $\beta^\prime = \beta + n - \dsum^n _{i=1} x_ i$ 
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item the Normalizing constant will be the Normalizing constant of the beta.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item the beta prior can be regarded as adding additional observations where $\alpha$ specifies the a-priori number of successes, and $\beta$ specifies the a-priori number of failures.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}
















\subsection{Bernoulli as Exponential Family}

\begin{enumerate}
    \item Bernoulli distribution: 
    $ P(x | \mu) = \mu^x(1 - \mu)^{1-x} $
    \hfill
    $ x \in \dCurlyBrac{0, 1} $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item Now:
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\[0.2cm]
    $
        \begin{aligned}
            P(x | \mu ) 
                &= \exp[\log (\mu ^x(1 - \mu )^{1-x})] 
                = \exp [x \log \mu  + (1 - x) \log(1 - \mu )] \\
                &= \exp [x \log \mu  - x \log(1 - \mu ) + \log(1 - \mu )]
                = \exp \dSquareBrac{ x \log \dfrac{\mu } {1-\mu } + \log(1 - \mu )} 
        \end{aligned}
    $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\[0.2cm]
    This can be identified as being in exponential family form by observing that:
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \begin{multicols}{2}
    \begin{enumerate}
        \item $ h(x) = 1  $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item $ \theta = \dfrac{\mu } {1-\mu } $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item $ \phi(x) = x  $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item $ A(\theta ) = - \log(1 - \mu ) = \log(1 + \exp(\theta )) $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}
    \end{multicols}

    \item $ \mu = \dfrac{1}{1 + \exp(-\theta)} $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\[0.2cm]
    The relationship between the original Bernoulli parameter $\mu$ and the natural parameter $\theta$ is known as the \textbf{sigmoid} or \textbf{logistic function}.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item The canonical conjugate prior has the form:
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\[0.2cm]
    .\hfill
    $
        \begin{aligned}
            P(\mu | \alpha, \beta )
            &= \dfrac{\mu}{1-\mu} 
            \exp \dParenBrac{
                \alpha \log \dfrac{\mu}{1-\mu} 
                + (\beta  + \alpha) \log(1 - \mu) 
                - A_c(\gamma )
            } \\
            &= \exp [(\alpha - 1) \log \mu  + (\beta  - 1) \log(1 - \mu ) - A_c(\alpha, \beta )] \\
            &\propto \mu^{\alpha - 1}  (1 - \mu )^{\beta-1}
        \end{aligned}
    $
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\[0.2cm]
    where we defined $\gamma  := \begin{bmatrix}\alpha \\ \beta  + \alpha\end{bmatrix}$ and $h_c(\mu ) := \dfrac{\mu }{1 - \mu }$. 
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item conjugate prior is Beta distribution
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}




\subsection{Summary}

\begin{enumerate}
    \item \textbf{Notation}:
    $
        \mathcal{B}(p)
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Parameters}:
    $  {\displaystyle 0\leq p\leq 1}$
    \hspace{1cm}
    $ {\displaystyle q=1-p} $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Support}:
    $
         {\displaystyle k\in \{0,1\}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{PMF}:
    $
         {\displaystyle {\begin{cases}q=1-p&{\text{if }}k=0\\p&{\text{if }}k=1\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{CDF}:
    $
         {\displaystyle {\begin{cases}0&{\text{if }}k<0\\1-p&{\text{if }}0\leq k<1\\1&{\text{if }}k\geq 1\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Mean}:
    $
        p
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Median}:
    $
         {\displaystyle {\begin{cases}0&{\text{if }}p<1/2\\\left[0,1\right]&{\text{if }}p=1/2\\1&{\text{if }}p>1/2\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Mode}:
    $
         {\displaystyle {\begin{cases}0&{\text{if }}p<1/2\\0,1&{\text{if }}p=1/2\\1&{\text{if }}p>1/2\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Variance}:
    $
         {\displaystyle p(1-p)=pq}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Median absolute deviation (MAD)}:
    $
         {\displaystyle 2p(1-p)=2pq}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Skewness}:
    $
         {\displaystyle {\dfrac {q-p}{\sqrt {pq}}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Excess kurtosis}:
    $
         {\displaystyle {\dfrac {1-6pq}{pq}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Entropy}:
    $
         {\displaystyle -q\ln q-p\ln p}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Moment-generating function (MGF)}:
    $
         {\displaystyle q+pe^{t}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Characteristic function (CF)}:
    $
         {\displaystyle q+pe^{it}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Probability-generating function (PGF)}:
    $
         {\displaystyle q+pz}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Fisher information}:
    $
         {\displaystyle {\dfrac {1}{pq}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}
\end{enumerate}




