\section{Bernoulli distribution ($X \sim B(p)$)}



\subsection{Distribution of the Sample Statistic ($T_n$)}

\subsubsection{Distribution of the Sample Average}

\begin{enumerate}
    \item Let $X_1 , X_2, \cdots , X _n$ are i.i.d. Bernoulli distributed, $X _i \sim \mathcal{B} ( p)$, the sum of the random variables is binomial $\text{Bin} (n, p)$ distributed.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item This implies that the distribution function of the sample average $\bar{X}$ of Bernoulli distributed random variables in $x \in [0, 1]$ is given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $
        F _{\bar{X}} (x)
        = P ( \bar{X} \leq x)
        = P (X_1 + X_2 + \cdots + X _n \leq nx)
        = \dsum ^{\dfloor{nx}} _{k=0} \dbinom{n}{k} p ^k (1 - p)^{n-k}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}


\end{enumerate}




\subsection{Summary}

\begin{enumerate}
    \item \textbf{Notation}:
    $
        \mathcal{B}(p)
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Parameters}:
    $  {\displaystyle 0\leq p\leq 1}$
    \hspace{1cm}
    $ {\displaystyle q=1-p} $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Support}:
    $
         {\displaystyle k\in \{0,1\}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{PMF}:
    $
         {\displaystyle {\begin{cases}q=1-p&{\text{if }}k=0\\p&{\text{if }}k=1\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{CDF}:
    $
         {\displaystyle {\begin{cases}0&{\text{if }}k<0\\1-p&{\text{if }}0\leq k<1\\1&{\text{if }}k\geq 1\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Mean}:
    $
        p
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Median}:
    $
         {\displaystyle {\begin{cases}0&{\text{if }}p<1/2\\\left[0,1\right]&{\text{if }}p=1/2\\1&{\text{if }}p>1/2\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Mode}:
    $
         {\displaystyle {\begin{cases}0&{\text{if }}p<1/2\\0,1&{\text{if }}p=1/2\\1&{\text{if }}p>1/2\end{cases}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Variance}:
    $
         {\displaystyle p(1-p)=pq}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Median absolute deviation (MAD)}:
    $
         {\displaystyle 2p(1-p)=2pq}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Skewness}:
    $
         {\displaystyle {\dfrac {q-p}{\sqrt {pq}}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Excess kurtosis}:
    $
         {\displaystyle {\dfrac {1-6pq}{pq}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Entropy}:
    $
         {\displaystyle -q\ln q-p\ln p}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Moment-generating function (MGF)}:
    $
         {\displaystyle q+pe^{t}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Characteristic function (CF)}:
    $
         {\displaystyle q+pe^{it}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Probability-generating function (PGF)}:
    $
         {\displaystyle q+pz}
    $
    \hfill \cite{wiki/Bernoulli_distribution}

    \item \textbf{Fisher information}:
    $
         {\displaystyle {\dfrac {1}{pq}}}
    $
    \hfill \cite{wiki/Bernoulli_distribution}
\end{enumerate}





\subsection{Maximum Likelihood Estimation (MLE)}


\begin{enumerate}
    \item Let $X_1 , X_2, \cdots , X_ n$ having an i.i.d. Bernoulli $\mathcal{B} ( p)$ distribution.
    We would like to estimate the parameter $p$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If we obtain a realization $x_1 , x_2, \cdots , x _n$ , we could ask how likely it is that we observe this set of results for given values of $p$.
    This so-called \textbf{likelihood} of the data is given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $
        L ( p)
        = p^ {x_1} (1 - p)^{1-x_1}\ p ^{x_2} (1 - p)^{1-x_2} \cdots p ^{x_n} (1 - p)^{1-x_n}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the term $p^ {x _i} (1 - p)^{1-x_i}$ is the probability that the random variable $X _i$ will attain the realization $x _i$ , i.e. $P (X _i = x _i ) = p^ {x _i} (1 - p)^{1-x_i}$ , with $x_ i \in \dCurlyBrac{0, 1}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The product $L ( p)$ represents the probability that $X_1 , X_2, \cdots , X _n$ will attain the realization $x_1 , x_2, \cdots , x _n$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item exploiting the i.i.d. assumption:
    \begin{enumerate}
        \item we assume identical distributions for each random variable $X _i$ (and we thus work with parameters $p$ as opposed to $p_i$)
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item the probability of the joint observations is given by the product over the probability of the individual observations; this is possible by virtue of the independence assumption
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item Since the likelihood is a function of the parameter $p$, we can search for a $p$ that would maximize the likelihood.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The maximum likelihood estimate $\hat{p}$ is the value that maximizes $L ( p)$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item To obtain this maximum, it is more convenient to take the logarithm of the likelihood $\ell( p)$ given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $
         \ell(p)
         = \dsum ^n _{i=1} [x _i \log ( p) + (1 - x_ i ) \log (1 - p)]
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    The logarithm of a function achieves its maximum value at the same points as the function itself, but is \textbf{easier} to work with analytically as we can replace the multiplications by sums that are easier to differentiate.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the maximum of a function can be obtained by taking the derivative and then equating it to zero and solving the equation for the variable of interest
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        \begin{aligned}
            &\dfrac{d \ell(p)}{dp}
            = \ell^\prime ( p)
            = \dsum ^n _{i=1} \dParenBrac{\dfrac{x _i}{p} - \dfrac{1 - x_ i} {1 - p}} = 0
            \Leftrightarrow (1 - p) \dsum^n _{i=1} x_ i - np + p \dsum^n _{i=1} x_ i = 0 \\
            &\Leftrightarrow np = \dsum^n _{i=1} x_ i
            \Leftrightarrow p = \dfrac{1}{n} \dsum^n _{i=1} x_ i = \bar{x}
        \end{aligned}
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    $

    \item The maximum likelihood estimate is now given by $\hat{p} = \bar{x}$.
    Since any value $\bar{x} - \varepsilon,\ \varepsilon > 0$, for $p$ in the derivative gives a positive value the solution is a maximum.
    The ML estimator is now $\hat{p} = \bar{X}$, which is the same as the MME, since the first moment of a Bernoulli random variable is $p$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}


\end{enumerate}



\subsection{Estimating the Parameter using Bayesian Methods}

\begin{enumerate}
    \item Let $\theta$ denote the probability of heads when throwing a (possibly unfair) coin, and let us treat the observations $X_1, \cdots , X_n$ from the coin as i.i.d. $\mathcal{B}(\theta)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Using $x$ to denote the vector of all $x_i$ realizations, we have:
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        \ell(\theta) 
        = P(x|\theta)
        = \dprod^n_{ i=1} \theta^{x _i} (1 - \theta)^{1-x_i}
        = \theta^{[\tsum^n _{i=1} x_i]} (1 - \theta)^{[n-\tsum^n_{i=1} x_i ]}
        = \theta^{n \bar{x} _n} (1 - \theta)^{[n(1- \bar{x}_ n )]}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Let prior be the support (i.e., $f (\theta) > 0$) for all plausible values of $\theta$.
    As in our case $\theta$ is a Bernoulli $p$ for which $0 \leq p \leq 1$ we should choose $f (\theta)$ such that it has support on $[0, 1]$. 
    It seems reasonable, in the absence of any other information, to choose a prior distribution that gives equal likelihood to all plausible values of $\theta$: the uniform distribution function on $[0, 1]$ thus seems a reasonable choice. 
    The uniform on $[0, 1]$ is however a special case of a much more flexible distribution function called the \textbf{beta distribution} function.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The $\text{Beta}(\alpha, \beta)$ distribution function is a continuous distribution function with PDF: ($\alpha > 0$ and $\beta > 0$)
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        f (\theta) = \dfrac{\Gamma (\alpha + \beta) }{\Gamma (\alpha)\ \Gamma (\beta)}\ \theta^{\alpha-1}(1 - \theta)^{\beta-1}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.1cm]
    for $0 < \theta < 1$ and zero otherwise.
    Gamma function $\Gamma (x) =\dint ^\infty _0 s^ {x-1}\ e^{-s}\ ds$.
    When we choose $\alpha = 1$ and $\beta = 1$ the resulting beta distribution function is actually a \textbf{uniform distribution} function on $[0, 1]$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  The $\text{Beta}(1,1)$ is considered a relatively \textbf{uninformative prior} for the Bernoulli $p$.
    The density with its peak at $0.5$ is given by $\alpha = \beta = 5$; as long as both parameters of the beta are equal and larger than $1$ the maximum of the density will be $0.5$. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{posterior}:
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    .\hfill
    $
        \begin{aligned}
            f (\theta|x) 
            &\propto p(x|\theta) f (\theta) \\
            &= \theta^{(\tsum^n _{i=1} x_i)} (1 - \theta)^{(n-\tsum^n _{i=1} x _i)} \dfrac{\Gamma (\alpha + \beta) }{\Gamma (\alpha)\Gamma (\beta)} \theta ^{\alpha-1}(1 - \theta)^{\beta-1} \\
            &\propto \theta^{(\alpha+\tsum^n _{i=1} x _i )-1}(1 - \theta)^{ (\beta+n-\tsum^n_{ i=1} x_ i )-1}
        \end{aligned}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  the posterior $f (\theta|D)$ is proportional to the density of a $\text{Beta}(\alpha^\prime, \beta^\prime)$ distribution, with
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item $\alpha^\prime = \alpha + \dsum^n _{i=1} x_ i$
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item $\beta^\prime = \beta + n - \dsum^n _{i=1} x_ i$ 
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item the Normalizing constant will be the Normalizing constant of the beta.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item the beta prior can be regarded as adding additional observations where $\alpha$ specifies the a-priori number of successes, and $\beta$ specifies the a-priori number of failures.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}














