\section{Exponential Distribution ( $\text{Exponential}(\lambda)$ )}


\begin{table}[H]
    \hfill
    \begin{minipage}{0.45\linewidth}
        \begin{figure}[H]
            \centering
            \includegraphics[
                width=\linewidth,
                height=5cm,
                keepaspectratio,
            ]{images/distributions/Exponential_distribution_pdf_-_public_domain.svg.png}
            \caption{Exponential Distribution: PDF \cite{wiki/Exponential_distribution}}
        \end{figure}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \begin{figure}[H]
            \centering
            \includegraphics[
                width=\linewidth,
                height=5cm,
                keepaspectratio,
            ]{images/distributions/Exponential_distribution_cdf_-_public_domain.svg.png}
            \caption{Exponential Distribution: CDF \cite{wiki/Exponential_distribution}}
        \end{figure}
    \end{minipage}
    \hfill
\end{table}

\vspace{0.3cm}

\begin{enumerate}
    \item the class of distributions called the exponential family provides the right balance of generality while retaining favorable computation and inference properties.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item Which class of distributions have finite-dimensional sufficient statistics, that is the number of parameters needed to describe them does not increase arbitrarily?
    The answer is exponential family distributions
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
\end{enumerate}


\subsection{Distribution of the Sample Statistic ($T_n$)}

\begin{enumerate}
    \item Let $X_1 , X_2, \cdots , X _n$ are i.i.d. exponentially distributed, i.e. $X _i \sim \exp(\lambda )$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The exponential distribution function is given by $F (x) = 1 - \exp (-\lambda x)$ for $x > 0$ and zero otherwise.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}

\subsubsection{Distribution of the Sample Minimum ($T_n = F _{X_{(1)}}$)}

\begin{enumerate}
    \item $F _{X_{(1)}} (x) = 1 - \exp (-n\lambda x)$ for $x > 0$ and zero otherwise
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Thus this implies that the sample distribution of the minimum $X_{(1)}$ is exponentially distributed, $X_{(1)} \sim \exp (n\lambda)$, but now with parameter $n\lambda$, when $X_1 ,X_2, \cdots , X _n$ are i.i.d. $\exp(\lambda)$ distributed.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}



\subsection{Farlie–Gumbel–Morgenstern (FGM) Family}

\begin{enumerate}
    \item assume that $F_X$ and $F_Y$ are exponential CDFs with parameters $\lambda_X$ and $\lambda_Y$ , respectively
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $\mbbE[X (1 - 2F_X (X))] = -[2\lambda_X ]^{-1}$ and $\mbbE[Y (1 - 2F_Y (Y ))] = -[2\lambda_Y ]^{-1}$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $\tCov[X, Y] = -\dfrac{\alpha}{4\lambda_X \lambda_Y} $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Pearson’s correlation coefficient}: $\rho_P = -\dfrac{\alpha}{4}$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Pearson’s correlation coefficient estimator} ($r_P$):
    The dependency parameter $\alpha$ can be estimated by $r_P \bar{X} \bar{Y}$ .
    Indeed, $r_P$ estimates $\alpha\lambda^{-1}_X \lambda^{-1}_Y$ and the sample averages $\bar{X}$ and $\bar{Y}$ estimate the parameters $\lambda^{-1}_X$ and $\lambda^{-1}_Y$ , respectively.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}






\subsection{Exponential Family}

\begin{enumerate}
    \item An exponential family is a family of probability distributions, parameterized by $\bm{\theta} \in \mbbR^D$ , of the form
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    .\hfill
    $P(\bm{x} | \bm{\theta}) = h(\bm{x})\ \exp (\dAngleBrac{\bm{\theta}, \phi(\bm{x})} - A(\bm{\theta}))$
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    where $\phi(\bm{x})$ is the vector of sufficient statistics.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \\
    By Default: $ \dAngleBrac{\bm{\theta}, \phi(\bm{x})} = \bm{\theta}^\top \phi(\bm{x}) $ (standard dot product)
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \begin{enumerate}
        \item The factor $h(\bm{x})$ can be absorbed into the dot product term by adding another entry ($\log h(\bm{x})$) to the vector of sufficient statistics $\phi(\bm{x})$, and constraining the corresponding parameter $\theta_0 = 1$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The term $A(\bm{\theta})$ is the \textbf{normalization constant} that ensures that the distribution sums up or integrates to one and is called the \textbf{log-partition function}.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The parameters $\theta$ are called the \textbf{natural parameters}.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}

    \item Note that the form of the exponential family is essentially a particular expression of $g_\theta (\phi(x))$ in the Fisher-Neyman theorem.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item Exponential families provide a convenient way to find \textbf{conjugate pairs of distributions}. 
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \begin{enumerate}
        \item Consider the random variable $X$ is a member of the exponential family:
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \\[0.2cm]
        .\hfill
        $ P(\bm{x} | \bm{\theta }) = h(\bm{x}) \exp (\dAngleBrac{\bm{\theta }, \phi (\bm{x})} - A(\bm{\theta })) $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item Every member of the exponential family has a conjugate prior:
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \\[0.2cm]
        .\hfill
        $ 
            P(\bm{\theta } | \gamma ) 
            = h_c(\bm{\theta }) \exp \dParenBrac{\dAngleBrac{
                \begin{bmatrix} \gamma _1 \\ \gamma _2  \end{bmatrix} ,
                \begin{bmatrix} \bm{\theta } \\ -A(\bm{\theta }) \end{bmatrix}
            } - A_c(\gamma ) } 
        $
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \\[0.2cm]
        where $\gamma  = \begin{bmatrix} \gamma _1 \\ \gamma _2  \end{bmatrix}$ has dimension $\dim(\bm{\theta }) + 1$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item The sufficient statistics of the conjugate prior are $ \begin{bmatrix} \bm{\theta } \\ -A(\bm{\theta }) \end{bmatrix} $.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

        \item By using the knowledge of the general form of conjugate priors for exponential families, we can derive functional forms of conjugate priors corresponding to particular distributions.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}

    \item 
\end{enumerate}







\subsection{Summary}

\begin{enumerate}
    \item \textbf{Parameters}: ${\displaystyle \lambda >0}$ (rate or inverse scale)
    \hfill \cite{wiki/Exponential_distribution, statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Support}: ${\displaystyle x\in [0,\infty )}$
    \hfill \cite{wiki/Exponential_distribution, statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{PDF}:
    ${\displaystyle \lambda e^{-\lambda x}}$
    \hfill \cite{wiki/Exponential_distribution, statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{CDF}: ${\displaystyle 1-e^{-\lambda x}}$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Quantile}: ${\displaystyle -{\dfrac {\ln(1-p)}{\lambda }}}$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Mean}: $\dfrac{1}{\lambda}$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Median}: $\dfrac{\ln(2)}{\lambda}$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Mode}: $0$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Variance}: $\dfrac{1}{\lambda^2}$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Skewness}: $2$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Excess kurtosis}: $6$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Entropy}: $1-\ln(\lambda)$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Fisher information}: $\dfrac{1}{\lambda^2}$
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Expected shortfall}:
    $
        {\displaystyle {\dfrac {-\ln(1-p)+1}{\lambda }}}
    $
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Moment-generating function (MGF)}:
    $
        {\displaystyle {\dfrac {\lambda }{\lambda -t}},{\text{ for }}t<\lambda }
    $
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Characteristic function (CF)}:
    $
        {\displaystyle {\dfrac {\lambda }{\lambda -it}}}
    $
    \hfill \cite{wiki/Exponential_distribution}

    \item \textbf{Kullback–Leibler divergence}:
    $
        {\displaystyle \ln {\dfrac {\lambda _{0}}{\lambda }}+{\dfrac {\lambda }{\lambda _{0}}}-1}
    $
    \hfill \cite{wiki/Exponential_distribution}
\end{enumerate}





















