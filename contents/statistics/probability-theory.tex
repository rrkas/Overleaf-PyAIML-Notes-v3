\chapter{Probability Theory}

\section{Definitions}

\begin{enumerate}
    \item \textbf{Descriptive statistics} summarizes and visualizes the observed data. It is usually not very difficult, but it forms an essential part of reporting(scientific)results.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Inferential statistics} tries to draw conclusions from the data that would hold true for part or the whole of the population from which the data is collected.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The \textbf{theory of probability} makes it possible to connect the two disciplines of descriptive and inferential statistics.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item
    \begin{definition}[sample space ($\Omega$)]
        The sample space is the set of all possible outcomes of the experiment, usually denoted by $\Omega$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{definition}

    \item
    \begin{definition}[event space ($\mathcal{A}$)]
        The event space is the space of potential results of the experiment.
        A subset $A$ of the sample space $\Omega$ is in the event space $\mathcal{A}$ if at the end of the experiment we can observe whether a particular outcome $\omega \in \Omega$ is in $A$.
        The event space $\mathcal{A}$ is obtained by considering the collection of subsets of $\Omega$, and for discrete probability distributions $\mathcal{A}$ is often the power set of $\Omega$.
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{definition}

    \item An \textbf{event} is defined as something that happens or it is seen as a result of something.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Definition 1}: probability of an event $A$ for a finite population can be given by $P(A) = \dfrac{N_A}{N}$ with $N_A$ the number of units with characteristic $A$ and $N$ the size of the population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item This definition is only correct if each opportunity for the event to occur is as likely to produce the event as any other opportunity.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Another limitation of this definition is that it is defined for finite populations only.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item \textbf{Definition 2} (more theoretical): Probability is the proportion of the occurrence of an event obtained from infinitely many repeated and identical trials or experiments under similar conditions.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item \textbf{Example}: if a die is thrown $n$ times and the event $A$ is the single dot facing up, then the probability $P (A)$ of the event $A$ can be \textbf{approximated} by the ratio of the number of throws $n_A$ with a single dot facing up and the total number of throws $n$, i.e., $P(A) \approx \dfrac{n_A}{n}$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item When the number of repeated trials $n$ is increased it is expected that the proportion $\dfrac{n_A}{n}$ converges to some value $p$ (which would be equal to $1/6$ if the die is \textit{fair}).
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item $P(A) =\displaystyle \lim_{n\to \infty} \dfrac{n_A}{n} = p$
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item \textbf{Definition 3}: With each event $A \in \mathcal{A}$, we associate a number $P (A)$ that measures the probability or degree of belief that the event will occur.
    $P (A)$ is called the probability of $A$.
    \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

    \item we simply define the probability $P(A)$ of an event $A$ as an (unknown) value between zero and one, $0 \leq P (A) \leq 1$, where both boundaries are allowed, which could either be approximated by collecting appropriate and real data or by the limit of a proportion of repeated and identical trials.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item
    \begin{definition}[Observed probabilities]
        Observed probabilities are empirical probabilities — that is, probabilities calculated from the sample data.
    \end{definition}
    \begin{enumerate}
        \item These probabilities are \textbf{not theoretical}, but come directly from real-world observations.
    \end{enumerate}

    \item interpretations of probability:
    \begin{enumerate}
        \item \textbf{Frequentist Interpretation (Classical View)}
        \begin{enumerate}
            \item Probability is the \textbf{long-run frequency} of an event occurring, based on \textbf{repeated trials} under identical conditions.

            \item \textbf{Key Features}:
            \begin{enumerate}
                \item Probability is objective and based on data.

                \item An event's probability is defined by the limit of its relative frequency as the number of trials approaches infinity.

                \item Parameters (like the mean or proportion) are fixed but unknown.

                \item You don’t assign probabilities to hypotheses or parameters.
            \end{enumerate}
        \end{enumerate}

        \item \textbf{Bayesian Interpretation}
        \begin{enumerate}
            \item Probability is a measure of belief or \textbf{degree of certainty} about an event or statement, \textbf{given current knowledge}.

            \item \textbf{Key Features}:
            \begin{enumerate}
                \item Probability is subjective and depends on prior knowledge.
                \item You can assign probabilities to hypotheses, events, and parameters.
                \item Uses Bayes' Theorem to update beliefs based on new evidence.
                \item A parameter can be treated as a random variable with its own probability distribution.
            \end{enumerate}
        \end{enumerate}
    \end{enumerate}
\end{enumerate}


\section{Event Axioms}

\begin{enumerate}
    \item The complement of event $A$ is denoted by $A^c$ and it indicates that event $A$ does not occur.
    The probability that event $A$ occurs is one minus the probability that the event $A$ does not occur; thus $P (A) = 1 - P (A^c)$.
    This rule is based on the assumption that either event $A$ occurs or event $A^c$ occurs.
    This means that $P (A \cup A^c) = 1$, since we will see either $A$ or $A^c$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item
    \begin{definition}[joint event/ mutual event ($A \cap B$)]
        The occurrence of two events $A$ and $B$ at the same time is denoted by $A \cap B$.
        This is often referred to as the joint or mutual event.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}

    \item The event that either $A$ or $B$ (or both) occurs is denoted by $A \cup B$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The probability of no event must be zero.
    Not having events is indicated by the empty set $\varnothing$ and the probability is $P(\varnothing) = 0$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}


    \item
    \begin{definition}[mutually exclusive events ($P (A \cap B) = P (\varnothing) = 0$)]
        if two events $A$ and $B$ can never occur together (mutually exclusive events), then it follows that $A \cap B = \varnothing$ and $P (A \cap B) = P (\varnothing) = 0$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}

    \item
    \begin{definition}[independent events ($A \perp B$) ($P (A \cap B) = P (A) \cdot P (B)$)]
        We call two events $A$ and $B$ independent if and only if the probability of the mutual event is equal to the product of the probabilities of each event $A$ and $B$ separately.
        Thus the independence of events A and B (denoted by $A \perp B$) is equivalent with $P (A \cap B) = P (A) \cdot P (B)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}
    \begin{enumerate}
        \item any event $A$ with the non-event $\varnothing$ is independent: $P(A \cap \varnothing) = P(\varnothing) = 0 = 0 \cdot P(A) = P(\varnothing) P(A)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item  if two events with a positive probability ($P(A) > 0$ and $P(B) > 0$) that are also mutually exclusive can never be independent: $0 = P(\varnothing) = P(A \cap B) < P(A) P(B)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item
    \begin{definition}[associated events/ associated variables]
        Two events or variables are considered associated when they are not independent.
    \end{definition}
\end{enumerate}


\section{Event Rules}

\begin{enumerate}
    \item If the events $A$ and $B$ are independent, then the events $A$ and $B^c$, the events $A^c$ and $B$, and the events $A^c$ and $B^c$ are also independent.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The probability of the occurrence of either event $A$ or $B$ or both is equal to the sum of the probabilities of these events separately minus the probability that both events occur at the same time, i.e., $P (A \cup B) = P (A) + P (B) - P (A \cap B)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item For mutually exclusive events $A$ and $B$, $P (A \cup B) = P (A) + P (B)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item
    \begin{definition}[law of total probability ($P (A) = P (A \cap B) + P (A \cap B^c)$)]
        The probability of an event $A$ is the sum of the probability of both events $A$ and $B$ and the probability of both events $A$ and $B^c$, thus $P (A) = P (A \cap B) + P (A \cap B^c)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}
\end{enumerate}



\section{Conditional Probability ($P(A|B)$)}\label{statistics/probability-theory/Conditional Probability}


\begin{enumerate}
    \item In some situations probability statements are of interest for a particular subset of outcomes.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If event A represents the occurrence of a specific condition or outcome, and event B represents the occurrence of a related or influencing condition or characteristic, then the probability of interest is the conditional probability of A given B, denoted by $P(A|B)$.
    We refer to this conditional probability as the probability of event A \textit{given} event B.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $P(A|B) = \begin{cases}
        \dfrac{P(A\cap B)}{P(B)} & \text{if } P(B) > 0 \\
        0 & \text{if } P(B) = 0
    \end{cases}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item if event $B$ could never occur (i.e., $P (B) = 0$), there is no reason to define the conditional probability
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If we deal with two events $A$ and $B$, the relevant probabilities can be summarized in the a $2 \times 2$ contingency table.
    In column $A$ and row $B$, the probability of the occurrence of both events $A$ and $B$ at the same time is given by $P (A \cap B)$.
    Using the conditional relation, it can also be expressed by $P(A|B) P(B)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    \begin{customArrayStretch}{1.2}
        \begin{table}[H]
            \centering
            \begin{tabular}{|l||l|l|l|}
                  \hline
                  & $A$ & $A^c$ &  \\
                  \hline\hline

                  $B$ & $P (A \cap B) = P (A|B) P (B)$ & $P (A^c \cap B) = P (A^c|B) P (B)$ & $P (B)$ \\
                  \hline

                  $B^c$ & $P (A \cap B^c) = P (A|B^c) P (B^c)$ & $P (A^c \cap B^c) = P (A^c|B^c) P (B^c)$ & $P (B^c)$ \\
                  \hline

                  & $P (A)$ & $P (A^c)$ & $1$ \\
                  \hline
            \end{tabular}
            \caption{Conditional probabilities in a $2 \times 2$ contingency table \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
            \label{statistics/probability-theory/Conditional Probability/Conditional-probabilities-contingency-table}
        \end{table}
    \end{customArrayStretch}


    \item \textbf{Marginal Probability}: Marginal Probability refers to the probability of a single event occurring, without consideration of any other events.
    It is derived from a joint probability distribution and represents the likelihood of an event happening in isolation.
    \hfill \cite{geeksforgeeks/engineering-mathematics/marginal-probability}
    \begin{enumerate}
        \item
        $
            P(X=x) =
            \begin{cases}
                \dsum_y P(X=x, Y=y) & \text{ discrete} \\[0.2cm]
                \dint_{-\infty}^\infty f_{XY}(x,y)dy & \text{ continuous} \\
            \end{cases}
        $
        \hfill \cite{geeksforgeeks/engineering-mathematics/marginal-probability}

        \item
        $
            P(Y=y) =
            \begin{cases}
                \dsum_x P(X=x, Y=y) & \text{ discrete} \\[0.2cm]
                \dint_{-\infty}^\infty f_{XY}(x,y)dx & \text{ continuous} \\
            \end{cases}
        $
        \hfill \cite{geeksforgeeks/engineering-mathematics/marginal-probability}
    \end{enumerate}

    \item If we consider a change of variables $x = g(y)$, then a function $f (x)$ becomes $\tilde{f} (y) = f (g(y))$.
    Now consider a probability density $P_x(x)$ that corresponds to a density $P_y (y)$ with respect to the new variable $y$, where the suffices denote the fact that $P_x(x)$ and $P_y (y)$ are different densities.
    Observations falling in the range $(x,\ x + \delta x)$ will, for small values of $\delta x$, be transformed into the range $(y,\ y + \delta y)$ where $P_x(x)\delta x \simeq P_y (y)\delta y$, and hence:
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
    \\[0.2cm]
    .\hfill
    $
        P_y(y)
        = P_x(x) \dabs{\dfrac{dx}{dy}}
        = P_x(g(y)) \dabs{g^\prime(y)}
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
\end{enumerate}








\section{Bayesian probabilities}

\begin{enumerate}
    \item
    \begin{theorem}[Bayes Theorem]
        \[
            \underset{posterior}{\underbrace{P(B|A)}}
            = \dfrac{P(A\cap B)}{P(A)}
            = \dfrac{
                \overset{likelihood}{\overbrace{P(A|B)}}
                \overset{prior}{\overbrace{P(B)}}
            }{
                \underset{evidence}{\underbrace{P(A)}}
            }
            \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein,mfml/book/mml/Deisenroth-Faisal-Ong}}
        \]
    \end{theorem}
    \begin{enumerate}
        \item \textbf{Posterior} ($P(B|A)$):
        \begin{enumerate}
            \item \textbf{Meaning}: The probability of $B$ given that you have observed $A$.
            \hfill \cite{common/online/chatgpt}

            \item \textbf{Why it's important}: This is what you’re trying to compute or update — your updated belief about $B$ after seeing evidence $A$.
            \hfill \cite{common/online/chatgpt}

            \item It is the quantity of interest in Bayesian statistics because it expresses exactly what we are interested in, i.e., what we know about $B$ after having observed $A$.
            \hfill\cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \end{enumerate}

        \item \textbf{Likelihood} ($P(A|B)$):
        \begin{enumerate}
            \item \textbf{Meaning}: The probability of seeing evidence $A$ assuming $B$ is true.
            \hfill \cite{common/online/chatgpt}

            \item \textbf{Why it's important}: It reflects how compatible the data is with different hypotheses.
            \hfill \cite{common/online/chatgpt}

            \item \textbf{Note}: This is not a probability of $B$, but rather of the data $A$ under different conditions of $B$.
            \hfill \cite{common/online/chatgpt}

            \item describes how $B$ and $A$ are related, and in the likelihood case of discrete probability distributions, it is the probability of the data $A$ if we were to know the latent variable $B$.
            \hfill\cite{mfml/book/mml/Deisenroth-Faisal-Ong}

            \item The likelihood is sometimes also called the “\textbf{measurement model}”.
            \hfill\cite{mfml/book/mml/Deisenroth-Faisal-Ong}

            \item Note that the likelihood is not a distribution in $B$, but only in $A$.
            We call $P(A | B)$ either the “likelihood of $B$ (given $A$)” or the “probability of $A$ given $B$” but never the "likelihood of $A$".
            \hfill\cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \end{enumerate}

        \item \textbf{Prior} ($P(B)$):
        \begin{enumerate}
            \item \textbf{Meaning}: The initial belief or probability of $B$ before seeing any evidence.
            \hfill \cite{common/online/chatgpt}

            \item \textbf{Why it's important}: It encodes your assumptions or background knowledge.
            \hfill \cite{common/online/chatgpt}

            \item Encapsulates our subjective prior prior knowledge of the unobserved (latent) variable $B$ before observing any data.
            \hfill\cite{mfml/book/mml/Deisenroth-Faisal-Ong}

            \item We can choose any prior that makes sense to us, but it is critical to ensure that the prior has a nonzero pdf (or pmf) on all plausible $B$, even if they are very rare.
            \hfill\cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \end{enumerate}

        \item \textbf{Evidence / Marginal Likelihood} ($P(A)$):
        \begin{enumerate}
            \item
            $
                P(A)
                = \dint P(A|B) P(B) dB
                = \mathbb{E}_B[P(A|B)]
            $
            \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

            \item \textbf{Meaning}: The total probability of observing the data $A$, considering all possible values of $B$.
            \hfill \cite{common/online/chatgpt}

            \item \textbf{Why it's important}: It acts as a normalizing constant so that the posterior sums (or integrates) to 1.
            \hfill \cite{common/online/chatgpt}

            \item \textbf{Why it's not a prior}: Even though $A$ appears in $P(B|A)$, in the context of Bayes’ theorem, it’s the observed data, and not a belief about a variable.
            That’s why it’s called the evidence.
            \hfill \cite{common/online/chatgpt}

            \item the marginal likelihood is independent of $B$, and it ensures that the posterior $P(B | A)$ is normalized.
            \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}

            \item The marginal likelihood can also be interpreted as the expected likelihood where we take the expectation with respect to the prior $P(B)$.
            \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
        \end{enumerate}

        \item Bayes’ theorem allows us to invert the relationship between $B$ and $A$ given by the likelihood.
        Bayes’ theorem is also called the “\textbf{probabilistic inverse}.”
        \hfill \cite{mfml/book/mml/Deisenroth-Faisal-Ong}
    \end{enumerate}

    \item \textbf{maximum likelihood}: $\bm{w}$ is set to the value that maximizes the likelihood function $P(D|\bm{w})$.
    \begin{enumerate}
        \item This corresponds to choosing the value of $\bm{w}$ for which the probability of the observed data set is maximized.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

        \item One common criterion for determining the parameters in a probability distribution using an observed data set is to find the parameter values that maximize the likelihood function.
        This might seem like a strange criterion because, from our foregoing discussion of probability theory, it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameters.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

        \item The negative log of the likelihood function is called an \textbf{error function}.
        Because the negative logarithm is a monotonically decreasing function, maximizing the likelihood is equivalent to minimizing the error.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
    \end{enumerate}

    \item These are not events that can be repeated numerous times in order to define a notion of probability.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item Bayes' theorem is used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item We capture our assumptions about $\bm{w}$, before observing the data, in the form of a prior probability distribution $P(\bm{w})$.
    The effect of the observed data $D = \dCurlyBrac{t_1, \cdots , t_N }$ is expressed through the conditional probability $P(D|\bm{w})$.
    Bayes’ theorem, which takes the form
    $
        P(\bm{w}|D) = \dfrac{P(\bm{w})P(D|\bm{w})}{P(D)}
    $
    then allows us to evaluate the uncertainty in $\bm{w}$ after we have observed $D$ in the form of the posterior probability $P(\bm{w}|D)$.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
    \begin{enumerate}
        \item The quantity $P(D|\bm{w})$ on the right-hand side of Bayes' theorem is evaluated for the observed data set $D$ and can be viewed as a function of the parameter vector $\bm{w}$, in which case it is called the \textbf{likelihood function}.
        It expresses how probable the observed data set is for different settings of the parameter vector $\bm{w}$.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

        \item The denominator ($P(D)$) is the normalization constant, which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
        \\
        .\hfill
        $
            P(D) = \dint P(D|\bm{w})P(\bm{w})\ d\bm{w}
        $
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

        \item Likelihood is not a probability distribution over $\bm{w}$, and its integral with respect to $\bm{w}$ does not (necessarily) equal one.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

        \item We can state Bayes’ theorem in words:
        $
            posterior \propto likelihood \times prior
        $
        where all of these quantities are viewed as functions of $\bm{w}$.
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
    \end{enumerate}

    \item One common criticism of the Bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a reflection of any prior beliefs.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item Reducing the dependence on the prior is one motivation for so-called noninformative priors.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
\end{enumerate}








\section{Frequentist AND/VS Bayesian}

\begin{enumerate}
    \item In both the Bayesian and frequentist paradigms, the likelihood function $P(D|\bm{w})$ plays a central role.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item In a frequentist setting, $\bm{w}$ is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets $D$.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item From the Bayesian viewpoint there is only a single data set $D$ (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over $\bm{w}$.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item However, bias to prior lead to difficulties when comparing different models, and indeed Bayesian methods based on poor choices of prior can give poor results with high confidence.
    Frequentist evaluation methods offer some protection from such problems, and techniques such as cross-validation remain useful in areas such as model comparison.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}


\end{enumerate}








\section{Expectations and covariances}

\begin{multicols}{2}
\begin{enumerate}[series=rvrules]
    \item
    $
        \mathbb{E}[f] =
        \begin{cases}
            \dsum_x P(x)f(x) & \text{ discrete} \\[0.4cm]
            \dint_{-\infty}^\infty P(x)f(x)dx & \text{ continuous} \\
        \end{cases}
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        \mathbb{E}[f] \simeq
        \dfrac{1}{N} \dsum_{n=1}^N f(x_n)
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        \mathbb{E}_x[f(x, y)]
        = \dint f(x,y) P(x) dx
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        \mathbb{E}_x[f(x, y)|y]
        = \dint f(x,y) P(x|y) dx
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        var[f] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2]
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        var[f] = \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        \begin{aligned}
            \tCov[x,\ y]
                & = \mathbb{E}_{x,y} [(x - \mathbb{E}[x]) (y - \mathbb{E}[y])] \\
                & = \mathbb{E}_{x,y} [xy] - \mathbb{E}[x]\mathbb{E}[y]
        \end{aligned}
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item
    $
        \begin{aligned}
            \tCov[\bm{x},\ \bm{y}]
                & = \mathbb{E}_{\bm{x},\bm{y}} [(\bm{x} - \mathbb{E}[\bm{x}]) (\bm{y}^\top - \mathbb{E}[\bm{y}^\top])] \\
                & = \mathbb{E}_{\bm{x},\bm{y}} [\bm{xy}^\top] - \mathbb{E}[\bm{x}]\mathbb{E}[\bm{y}^\top]
        \end{aligned}
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item $ \tCov[x] \equiv \tCov[x,x] $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
\end{enumerate}
\end{multicols}






\section{Bootstrap}


\begin{enumerate}
    \item Suppose our original data set consists of N data points $\bm{X} = \dCurlyBrac{x_1, \cdots , x_N }$.
    We can create a new data set $\bm{X}_B$ by drawing $N$ points at random from $\bm{X}$, with replacement, so that some points in $\bm{X}$ may be replicated in $\bm{X}_B$, whereas other points in X may be absent from $\bm{X}_B$.
    This process can be repeated $L$ times to generate $L$ data sets each of size $N$ and each obtained by sampling from the original data set $\bm{X}$.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item The statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item One \textbf{advantage} of the Bayesian viewpoint is that the inclusion of prior knowledge arises naturally.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

    \item For instance, that a fair-looking coin is tossed three times and lands heads each time.
    A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land heads!
    By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion.
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}


\end{enumerate}











\section{Association Measures/ Measures of Risk}

\begin{enumerate}
    \item $D$: the event of interest, such as a disease, failure, or success (also referred to as the outcome or result).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein, common/online/chatgpt}

    \item $E$: the event representing a possible influencing factor, such as exposure, a risk factor, or treatment (also called an explanatory variable).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein, common/online/chatgpt}
\end{enumerate}

\begin{customArrayStretch}{1.2}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline

        \multirow{2}{*}{Exposure} &  \multicolumn{2}{|l|}{Outcome} & \multirow{2}{*}{TOTAL} \\
        \cline{2-3}
        & $D$ & $D^c$ & \\
        \hline

        $E$ & $P(D|E)$ & $P(D^c|E)$ & $P(E)$ \\ \hline

        $E^c$ & $P(D|E^c)$ & $P(D^c|E^c)$ & $P(E^c)$ \\ \hline

        TOTAL & $P(D)$ & $P(D^c)$ & $1$ \\ \hline

    \end{tabular}
    \caption{$2 \times 2$ contingency table for exposure-outcome}
    \label{statistics/probability-theory/Conditional Probability/contingency-table-exposure-outcome}
\end{table}
\end{customArrayStretch}

\subsection{Risk Difference/ Excess Risk ($ER = P(D|E) - P(D|E^c)$)}

\begin{enumerate}
    \item The risk difference or excess risk is an absolute measure of risk, since it is nothing more than the difference in the conditional probabilities, i.e.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \[
        ER = P(D|E) - P(D|E^c)
        \hfill
        \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \item The risk difference is based on an additive model, i.e., $P (D|E) = ER + P (D|E^c)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item It always lies between $-1$ and $1$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $
        \begin{aligned}
            ER = 0
            & \Longleftrightarrow
            P (D|E) = P (D|E^c) \\
            & \Longleftrightarrow
            P (E^c) P (D \cap E) = P (E) P (D \cap E^c) \\
            & \Longleftrightarrow
            [1 - P (E)] P (D \cap E) = P (E)[P(D) - P (D \cap E)] \\
            & \Longleftrightarrow
            P (D \cap E) = P (D) P (E)
        \end{aligned}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item it can be viewed as the excess number of cases ($D$) as a fraction of the population size.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item If the complete population (of size $N$) were to be exposed, the number of cases would be equal to $N \cdot P (D) = N \cdot P (D|E)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item If the complete population were unexposed the number of cases would be equal to $N \cdot P (D) = N \cdot P (D|E^c)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Thus the difference in these numbers of cases indicates how the number of cases were to change if a completely exposed population would change to a completely unexposed population.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}
\end{enumerate}

\begin{customArrayStretch}{1.2}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|p{14cm}|}
            \hline

            $ER < 0$ & exposure ($E$) is protective for the outcome
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline

            $ER = 0$ & the outcome ($D$) is independent of the exposure ($E$)
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline

            $ER > 0$ & there is a greater risk of the outcome when exposed ($E$) than when unexposed ($E^c$)
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
        \end{tabular}
    \end{table}
\end{customArrayStretch}




\subsection{Relative Risk ($RR = {P(D|E)}/{P(D|E^c)}$)}

\begin{enumerate}
    \item The relative risk would compare the two conditional probabilities $P (D|E)$ and $P (D|E^c)$ by taking the ratio, i.e.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \[
        RR = \dfrac{P(D|E)}{P(D|E^c)}
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \item It is common to take as denominator the risk of the outcome D for the unexposed group.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The relative risk is based on a multiplicative model, i.e., $P (D|E) = RR \cdot P (D|E^c)$.
\end{enumerate}



\begin{customArrayStretch}{1.2}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|p{14cm}|}
            \hline

            $RR < 1$ & unexposed group has a higher probability of the outcome
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline

            $RR = 1$ & outcome and exposure are independent
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline

            $RR > 1$ & exposed group has a higher probability of the outcome ($D$) than the unexposed one
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
        \end{tabular}
    \end{table}
\end{customArrayStretch}



\subsection{Odds Ratio ($OR = ({P (D^c|E^c)}/{P (D^c|E)}) \times RR$)}

\begin{enumerate}
    \item
    \begin{definition}[Odds]
        The odds is a measure of how likely the outcome occurs with respect to not observing this outcome.
        The odds comes from gambling, where profits of bets are expressed as $1$ to $x$.
        For instance, the odds of $1$ to $n$ means that it is $n$ times more likely to loose than to win.
        The odds can be defined mathematically by $O = \dfrac{p}{(1 - p)}$, with $p$ the probability of winning.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}

    \item The odds ratio compares the odds for the exposed group with the odds for the unexposed group.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds of the exposed group is $O_E = \dfrac{P (D|E)}{1 - P (D|E)}$ and the odds for the unexposed group is $O_{E^c} = \dfrac{P (D|E^c)}{1 - P (D|E^c)}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds ratio is given by:
    \[
        OR
        = \dfrac{O_E}{O_{E^c}}
        = \dfrac{P (D|E)[1 - P (D|E^c)]}{P (D|E^c)[1 - P (D|E)] }
        = \dfrac{P (D^c|E^c)}{P (D^c|E)} \times RR
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \item  it is common to use the unexposed group as reference group, which implies that the odds of the unexposed group $O_{E^c}$ is used in the denominator.
\end{enumerate}


\begin{customArrayStretch}{1.2}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|p{14cm}|}
            \hline

            $OR < 1$ & unexposed group has a higher probability of the outcome
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline

            $OR = 1$ & outcome is independent of the exposure
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline

            $OR > 1$ & exposed group has a higher odds than the unexposed group, which implies that the exposed group has a higher probability of outcome $D$
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
        \end{tabular}
    \end{table}
\end{customArrayStretch}



\subsection{Relation in ER, RR \& OR}

\begin{enumerate}
    \item the odds ratio and relative risk are always ordered
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item odds ratio is always further away from 1 than the relative risk
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill
    $1 < RR < OR$
    \hfill \textbf{OR} \hfill
    $OR < RR < 1$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Proof:
    \begin{enumerate}
        \item  If $RR > 1$, we have that $P(D|E) > P(D|E^c)$, using its definition.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Since $P(D^c|E) = 1 - P(D|E)$ and $P(D^c|E^c) = 1 - P(D|E^c)$, we obtain that $P(D^c|E) < P(D^c|E^c)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Combining this inequality with the relation in Odds Ratio equation, we see that $OR > RR$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \end{enumerate}

    \item the odds ratio and relative risk are equal to each other when $RR = 1$ (or $OR = 1$).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds ratio is often considered more complex than the relative risk, in particular because of the simplicity of interpretation of the relative risk.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds ratio is, however, more frequently used in practice than the relative risk.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item An important reason for this is that the odds ratio is symmetric in exposure $E$ and outcome $D$.
        If the roles of the exposure and outcome are interchanged the odds ratio does not change, but the relative risk does.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \end{enumerate}
\end{enumerate}



















\section{Simpson’s Paradox}

\begin{enumerate}
    \item Simpson’s Paradox is when a trend or relationship that appears in separate groups of data reverses or disappears when the groups are combined.
    \hfill \cite{common/online/chatgpt}

    \item In other words:
    \begin{enumerate}
        \item A conclusion that seems true when you look at each group individually
        \hfill \cite{common/online/chatgpt}

        \item Can be completely misleading or even opposite when you combine the data.
        \hfill \cite{common/online/chatgpt}
    \end{enumerate}

    \item Why does it happen? Because of confounding variables — something else (like unequal group sizes) is influencing the results.
    \hfill \cite{common/online/chatgpt}

    \item Simpson demonstrated that the association between $D$ and $E$ in the collapsed contingency table is preserved in the two separate contingency tables for $C$ and $C^c$ whenever one or both of the following restrictions hold true:
    \[
        P (D \cap  E \cap  C) P (D \cap  E^c \cap  C^c) = P (D \cap  E^c \cap  C) P (D \cap  E \cap  C^c)
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]
    \[
        P (D \cap  E \cap  C) P(D^c \cap  E \cap  C^c)= P(D^c \cap  E \cap  C)P(D \cap  E \cap  C^c)
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \begin{enumerate}
        \item The first equation implies that the odds ratio for having the exposure $E$ for the presence or absence of $C$ in the outcome group $D$ is equal to one, i.e.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \[
            OR_{EC|D} = \dfrac{P (E|C, D)[1 - P (E|C^c, D)]}{P (E|C^c, D)[1 - P (E|C, D)]} = 1
            \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \]
        Thus $E$ and $C$ must be independent in the outcome group $D$, which means that $P (E \cap C|D) = P (E|D) P (C|D)$.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}

        \item The second equation implies that the odds ratio for the outcome $D$ in the presence or absence of $C$ in the exposed group $E$ is equal to one, i.e.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \[
            OR_{DC|E} = \dfrac{P (D|C, E)[1 - P (D|C^c, E)]}{P (D|C^c, E)[1 - P (D|C, E)]} = 1
            \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \]
        Thus this means that $D$ and $C$ are independent in the exposed group $E$, which means that $P (D \cap C|E) = P (D|E) P (C|E)$.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \end{enumerate}

    \item If the two independence requirements are violated, the event C is called a \textbf{confounder}.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item To say that C is not a confounder, we would want:
        \hfill \cite{common/online/chatgpt}
        \begin{enumerate}
            \item C is independent of E (i.e., the confounder is not related to the exposure): $C\perp E$
            \hfill \cite{common/online/chatgpt}

            \item C is independent of D given E (i.e., once we account for exposure, the confounder does not affect disease): $C \perp D | E$
            \hfill \cite{common/online/chatgpt}
        \end{enumerate}

        \item If C is related to both E and D — i.e.,
        \hfill \cite{common/online/chatgpt}
        \begin{enumerate}
            \item C affects the likelihood of being exposed (E), and
            \hfill \cite{common/online/chatgpt}

            \item C also affects the chance of getting the disease (D) independently of E,
            \hfill \cite{common/online/chatgpt}
        \end{enumerate}
        Then the independence assumptions are violated. Therefore, C is a confounder — because it distorts the observed relationship between E and D.
        \hfill \cite{common/online/chatgpt}
    \end{enumerate}
\end{enumerate}

\textbf{Example}:

\begin{customArrayStretch}{1.2}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        \multirow{2}{*}{Exposure} & \multicolumn{2}{c|}{Kidney Stones Outcome $\leq$ 2cm ($C$)} & \multirow{2}{*}{Total}\\
        \cline{2-3}
        & Removal ($D$) & No Removal ($D^c$) & \\
        \hline
        Nephrolithotomy ($E$) & 234 & 36 & 270 \\ \hline
        Open Surgery($E^c$) & 81 & 6 & 87 \\ \hline
        Total & 315 & 42 & 357 \\

        \hline \hline

        \hline
        \multirow{2}{*}{Exposure} & \multicolumn{2}{c|}{Kidney Stones Outcome $>$ 2cm ($C^c$)} & \multirow{2}{*}{Total}\\
        \cline{2-3}
        & Removal ($D$) & No Removal ($D^c$) & \\
        \hline

        Nephrolithotomy ($E$) & 55 & 25 & 80 \\ \hline
        Open Surgery($E^c$) & 192 & 71 & 263 \\ \hline
        Total & 247 & 96 & 343 \\ \hline
    \end{tabular}
    \caption{
        $2 \times 2$ contingency table for removal of kidney stones and two surgical treatments by size of kidney stones.
        \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    }
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        \multirow{2}{*}{Exposure} & \multicolumn{2}{c|}{Kidney Stones Outcome} & \multirow{2}{*}{Total}\\
        \cline{2-3}
        & Removal ($D$) & No Removal ($D^c$) & \\
        \hline

        Nephrolithotomy ($E$) & 289 & 61 & 350 \\ \hline
        Open surgery ($E^c$) & 273 & 77 & 350 \\ \hline
        Total & 562 & 138 & 700 \\ \hline
    \end{tabular}
    \caption{
        $2 \times 2$ contingency table for removal of kidney stones and two surgical treatments
        \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    }
\end{table}
\end{customArrayStretch}


\begin{enumerate}
    \item $RR = (289/350)/(273/350) = 1.0586$.  This means that percutaneous nephrolithotomy increases the “risk” of successful removal of the kidney stones with respect to open surgery.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $RR_{\leq 2} = 0.9309$ and $RR_{>2} = 0.9417$. it seems that open surgery has a higher success of kidney stone removal than percutaneous nephrolithotomy for both small and large stones.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item This is a contradiction.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}










