\chapter{Probability Theory}


\begin{enumerate}
    \item \textbf{Descriptive statistics} summarizes and visualizes the observed data. It is usually not very difficult, but it forms an essential part of reporting(scientific)results.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Inferential statistics} tries to draw conclusions from the data that would hold true for part or the whole of the population from which the data is collected.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The \textbf{theory of probability} makes it possible to connect the two disciplines of descriptive and inferential statistics.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

\end{enumerate}


\section{Definitions}

\begin{enumerate}
    \item An \textbf{event} is defined as something that happens or it is seen as a result of something.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Definition 1}: probability of an event $A$ for a finite population can be given by $P(A) = \dfrac{N_A}{N}$ with $N_A$ the number of units with characteristic $A$ and $N$ the size of the population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item This definition is only correct if each opportunity for the event to occur is as likely to produce the event as any other opportunity.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
        
        \item Another limitation of this definition is that it is defined for finite populations only.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item \textbf{Definition 2} (more theoretical): Probability is the proportion of the occurrence of an event obtained from infinitely many repeated and identical trials or experiments under similar conditions. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item \textbf{Example}: if a die is thrown $n$ times and the event $A$ is the single dot facing up, then the probability $P (A)$ of the event $A$ can be \textbf{approximated} by the ratio of the number of throws $n_A$ with a single dot facing up and the total number of throws $n$, i.e., $P(A) \approx \dfrac{n_A}{n}$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item When the number of repeated trials $n$ is increased it is expected that the proportion $\dfrac{n_A}{n}$ converges to some value $p$ (which would be equal to $1/6$ if the die is \textit{fair}).
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item $P(A) =\displaystyle \lim_{n\to \infty} \dfrac{n_A}{n} = p$
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}
    
    \item we simply define the probability $P(A)$ of an event $A$ as an (unknown) value between zero and one, $0 \leq P (A) \leq 1$, where both boundaries are allowed, which could either be approximated by collecting appropriate and real data or by the limit of a proportion of repeated and identical trials.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item 
    \begin{definition}[Observed probabilities]
        Observed probabilities are empirical probabilities — that is, probabilities calculated from the sample data.
    \end{definition}
    \begin{enumerate}
        \item These probabilities are \textbf{not theoretical}, but come directly from real-world observations.
    \end{enumerate}
\end{enumerate}


\section{Event Axioms}

\begin{enumerate}
    \item The complement of event $A$ is denoted by $A^c$ and it indicates that event $A$ does not occur.
    The probability that event $A$ occurs is one minus the probability that the event $A$ does not occur; thus $P (A) = 1 - P (A^c)$.
    This rule is based on the assumption that either event $A$ occurs or event $A^c$ occurs.
    This means that $P (A \cup A^c) = 1$, since we will see either $A$ or $A^c$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item 
    \begin{definition}[joint event/ mutual event ($A \cap B$)]
        The occurrence of two events $A$ and $B$ at the same time is denoted by $A \cap B$. 
        This is often referred to as the joint or mutual event. 
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}

    \item The event that either $A$ or $B$ (or both) occurs is denoted by $A \cup B$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The probability of no event must be zero. 
    Not having events is indicated by the empty set $\varnothing$ and the probability is $P(\varnothing) = 0$. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    
    \item 
    \begin{definition}[mutually exclusive events ($P (A \cap B) = P (\varnothing) = 0$)]
        if two events $A$ and $B$ can never occur together (mutually exclusive events), then it follows that $A \cap B = \varnothing$ and $P (A \cap B) = P (\varnothing) = 0$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}

    \item 
    \begin{definition}[independent events ($A \perp B$) ($P (A \cap B) = P (A) \cdot P (B)$)]
        We call two events $A$ and $B$ independent if and only if the probability of the mutual event is equal to the product of the probabilities of each event $A$ and $B$ separately.
        Thus the independence of events A and B (denoted by $A \perp B$) is equivalent with $P (A \cap B) = P (A) \cdot P (B)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}
    \begin{enumerate}
        \item any event $A$ with the non-event $\varnothing$ is independent: $P(A \cap \varnothing) = P(\varnothing) = 0 = 0 \cdot P(A) = P(\varnothing) P(A)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item  if two events with a positive probability ($P(A) > 0$ and $P(B) > 0$) that are also mutually exclusive can never be independent: $0 = P(\varnothing) = P(A \cap B) < P(A) P(B)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item 
    \begin{definition}[associated events/ associated variables]
        Two events or variables are considered associated when they are not independent.
    \end{definition}
\end{enumerate}


\section{Event Rules}

\begin{enumerate}
    \item If the events $A$ and $B$ are independent, then the events $A$ and $B^c$, the events $A^c$ and $B$, and the events $A^c$ and $B^c$ are also independent. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The probability of the occurrence of either event $A$ or $B$ or both is equal to the sum of the probabilities of these events separately minus the probability that both events occur at the same time, i.e., $P (A \cup B) = P (A) + P (B) - P (A \cap B)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item For mutually exclusive events $A$ and $B$, $P (A \cup B) = P (A) + P (B)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item 
    \begin{definition}[law of total probability ($P (A) = P (A \cap B) + P (A \cap B^c)$)]
        The probability of an event $A$ is the sum of the probability of both events $A$ and $B$ and the probability of both events $A$ and $B^c$, thus $P (A) = P (A \cap B) + P (A \cap B^c)$. 
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}    
    \end{definition}
\end{enumerate}



\section{Conditional Probability ($P(A|B)$)}\label{statistics/probability-theory/Conditional Probability}


\begin{enumerate}
    \item In some situations probability statements are of interest for a particular subset of outcomes.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If event A represents the occurrence of a specific condition or outcome, and event B represents the occurrence of a related or influencing condition or characteristic, then the probability of interest is the conditional probability of A given B, denoted by $P(A|B)$.
    We refer to this conditional probability as the probability of event A \textit{given} event B.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $P(A|B) = \begin{cases}
        \dfrac{P(A\cap B)}{P(B)} & \text{if } P(B) > 0 \\
        0 & \text{if } P(B) = 0
    \end{cases} 
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item if event $B$ could never occur (i.e., $P (B) = 0$), there is no reason to define the conditional probability
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If we deal with two events $A$ and $B$, the relevant probabilities can be summarized in the a $2 \times 2$ contingency table. 
    In column $A$ and row $B$, the probability of the occurrence of both events $A$ and $B$ at the same time is given by $P (A \cap B)$.
    Using the conditional relation, it can also be expressed by $P(A|B) P(B)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    \begin{customArrayStretch}{1.2}
        \begin{table}[H]
            \centering
            \begin{tabular}{|l||l|l|l|}
                  \hline
                  & $A$ & $A^c$ &  \\
                  \hline\hline
                  
                  $B$ & $P (A \cap B) = P (A|B) P (B)$ & $P (A^c \cap B) = P (A^c|B) P (B)$ & $P (B)$ \\
                  \hline
                  
                  $B^c$ & $P (A \cap B^c) = P (A|B^c) P (B^c)$ & $P (A^c \cap B^c) = P (A^c|B^c) P (B^c)$ & $P (B^c)$ \\
                  \hline
                  
                  & $P (A)$ & $P (A^c)$ & $1$ \\
                  \hline
            \end{tabular}
            \caption{Conditional probabilities in a $2 \times 2$ contingency table \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
            \label{statistics/probability-theory/Conditional Probability/Conditional-probabilities-contingency-table}
        \end{table}
    \end{customArrayStretch}

    \item 
    \begin{theorem}[Bayes Theorem]
        \[
            P(B|A) 
            = \dfrac{P(A\cap B)}{P(A)} = \dfrac{P(A|B) P(B)}{P(A)}
            \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \]
    \end{theorem}
    \begin{enumerate}
        \item \textbf{prior probability} ($P(A)$): probability available before we observe Event $B$
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}

        \item \textbf{posterior probability} ($P(B|A)$): it is the probability obtained after we have observed $A$
        \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
    \end{enumerate}

    \item \textbf{Marginal Probability}: Marginal Probability refers to the probability of a single event occurring, without consideration of any other events. 
    It is derived from a joint probability distribution and represents the likelihood of an event happening in isolation.
    \hfill \cite{geeksforgeeks/engineering-mathematics/marginal-probability}
    \begin{enumerate}
        \item 
        $
            P(X=x) = 
            \begin{cases}
                \dsum_y P(X=x, Y=y) & \text{ discrete} \\[0.2cm]
                \dint_{-\infty}^\infty f_{XY}(x,y)dy & \text{ continuous} \\
            \end{cases}
        $
        \hfill \cite{geeksforgeeks/engineering-mathematics/marginal-probability}

        \item 
        $
            P(Y=y) = 
            \begin{cases}
                \dsum_x P(X=x, Y=y) & \text{ discrete} \\[0.2cm]
                \dint_{-\infty}^\infty f_{XY}(x,y)dx & \text{ continuous} \\
            \end{cases}
        $
        \hfill \cite{geeksforgeeks/engineering-mathematics/marginal-probability}
    \end{enumerate}

    \item If we consider a change of variables $x = g(y)$, then a function $f (x)$ becomes $\tilde{f} (y) = f (g(y))$. 
    Now consider a probability density $P_x(x)$ that corresponds to a density $P_y (y)$ with respect to the new variable $y$, where the suffices denote the fact that $P_x(x)$ and $P_y (y)$ are different densities. 
    Observations falling in the range $(x,\ x + \delta x)$ will, for small values of $\delta x$, be transformed into the range $(y,\ y + \delta y)$ where $P_x(x)\delta x \simeq P_y (y)\delta y$, and hence:
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
    \\
    $
        P_y(y) 
        = P_x(x) \dabs{\dfrac{dx}{dy}} 
        = P_x(g(y)) \dabs{g^\prime(y)}
    $
    \hfill \cite{ml/book/Pattern-Recognition-And-Machine-Learning/Christopher-M-Bishop}
\end{enumerate}




\section{Association Measures/ Measures of Risk}

\begin{enumerate}
    \item $D$: the event of interest, such as a disease, failure, or success (also referred to as the outcome or result).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein, common/online/chatgpt}

    \item $E$: the event representing a possible influencing factor, such as exposure, a risk factor, or treatment (also called an explanatory variable).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein, common/online/chatgpt}
\end{enumerate}

\begin{customArrayStretch}{1.2}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline

        \multirow{2}{*}{Exposure} &  \multicolumn{2}{|l|}{Outcome} & \multirow{2}{*}{TOTAL} \\
        \cline{2-3}
        & $D$ & $D^c$ & \\
        \hline

        $E$ & $P(D|E)$ & $P(D^c|E)$ & $P(E)$ \\ \hline
        
        $E^c$ & $P(D|E^c)$ & $P(D^c|E^c)$ & $P(E^c)$ \\ \hline
    
        TOTAL & $P(D)$ & $P(D^c)$ & $1$ \\ \hline

    \end{tabular}
    \caption{$2 \times 2$ contingency table for exposure-outcome}
    \label{statistics/probability-theory/Conditional Probability/contingency-table-exposure-outcome}
\end{table}
\end{customArrayStretch}

\subsection{Risk Difference/ Excess Risk ($ER = P(D|E) - P(D|E^c)$)}

\begin{enumerate}
    \item The risk difference or excess risk is an absolute measure of risk, since it is nothing more than the difference in the conditional probabilities, i.e.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \[
        ER = P(D|E) - P(D|E^c)
        \hfill 
        \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \item The risk difference is based on an additive model, i.e., $P (D|E) = ER + P (D|E^c)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item It always lies between $-1$ and $1$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $
        \begin{aligned}
            ER = 0 
            & \Longleftrightarrow 
            P (D|E) = P (D|E^c) \\
            & \Longleftrightarrow 
            P (E^c) P (D \cap E) = P (E) P (D \cap E^c) \\
            & \Longleftrightarrow 
            [1 - P (E)] P (D \cap E) = P (E)[P(D) - P (D \cap E)] \\
            & \Longleftrightarrow 
            P (D \cap E) = P (D) P (E)
        \end{aligned}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item it can be viewed as the excess number of cases ($D$) as a fraction of the population size. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item If the complete population (of size $N$) were to be exposed, the number of cases would be equal to $N \cdot P (D) = N \cdot P (D|E)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item If the complete population were unexposed the number of cases would be equal to $N \cdot P (D) = N \cdot P (D|E^c)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Thus the difference in these numbers of cases indicates how the number of cases were to change if a completely exposed population would change to a completely unexposed population.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}
\end{enumerate}

\begin{customArrayStretch}{1.2}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|p{14cm}|}
            \hline
    
            $ER < 0$ & exposure ($E$) is protective for the outcome 
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
            
            $ER = 0$ & the outcome ($D$) is independent of the exposure ($E$) 
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
            
            $ER > 0$ & there is a greater risk of the outcome when exposed ($E$) than when unexposed ($E^c$) 
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
        \end{tabular}
    \end{table}
\end{customArrayStretch}




\subsection{Relative Risk ($RR = {P(D|E)}/{P(D|E^c)}$)}

\begin{enumerate}
    \item The relative risk would compare the two conditional probabilities $P (D|E)$ and $P (D|E^c)$ by taking the ratio, i.e.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \[
        RR = \dfrac{P(D|E)}{P(D|E^c)}
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \item It is common to take as denominator the risk of the outcome D for the unexposed group.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The relative risk is based on a multiplicative model, i.e., $P (D|E) = RR \cdot P (D|E^c)$.
\end{enumerate}



\begin{customArrayStretch}{1.2}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|p{14cm}|}
            \hline
    
            $RR < 1$ & unexposed group has a higher probability of the outcome
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
            
            $RR = 1$ & outcome and exposure are independent 
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
            
            $RR > 1$ & exposed group has a higher probability of the outcome ($D$) than the unexposed one 
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
        \end{tabular}
    \end{table}
\end{customArrayStretch}



\subsection{Odds Ratio ($OR = ({P (D^c|E^c)}/{P (D^c|E)}) \times RR$)}

\begin{enumerate}
    \item  
    \begin{definition}[Odds]    
        The odds is a measure of how likely the outcome occurs with respect to not observing this outcome.
        The odds comes from gambling, where profits of bets are expressed as $1$ to $x$. 
        For instance, the odds of $1$ to $n$ means that it is $n$ times more likely to loose than to win.
        The odds can be defined mathematically by $O = \dfrac{p}{(1 - p)}$, with $p$ the probability of winning.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{definition}

    \item The odds ratio compares the odds for the exposed group with the odds for the unexposed group.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds of the exposed group is $O_E = \dfrac{P (D|E)}{1 - P (D|E)}$ and the odds for the unexposed group is $O_{E^c} = \dfrac{P (D|E^c)}{1 - P (D|E^c)}$. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds ratio is given by:
    \[
        OR 
        = \dfrac{O_E}{O_{E^c}} 
        = \dfrac{P (D|E)[1 - P (D|E^c)]}{P (D|E^c)[1 - P (D|E)] }
        = \dfrac{P (D^c|E^c)}{P (D^c|E)} \times RR
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \item  it is common to use the unexposed group as reference group, which implies that the odds of the unexposed group $O_{E^c}$ is used in the denominator.
\end{enumerate}


\begin{customArrayStretch}{1.2}
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|p{14cm}|}
            \hline
    
            $OR < 1$ & unexposed group has a higher probability of the outcome
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
            
            $OR = 1$ & outcome is independent of the exposure
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
            
            $OR > 1$ & exposed group has a higher odds than the unexposed group, which implies that the exposed group has a higher probability of outcome $D$
            \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein} \\
            \hline
        \end{tabular}
    \end{table}
\end{customArrayStretch}



\subsection{Relation in ER, RR \& OR}

\begin{enumerate}
    \item the odds ratio and relative risk are always ordered
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item odds ratio is always further away from 1 than the relative risk
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\
    .\hfill 
    $1 < RR < OR$ 
    \hfill \textbf{OR} \hfill 
    $OR < RR < 1$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Proof: 
    \begin{enumerate}
        \item  If $RR > 1$, we have that $P(D|E) > P(D|E^c)$, using its definition. 
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Since $P(D^c|E) = 1 - P(D|E)$ and $P(D^c|E^c) = 1 - P(D|E^c)$, we obtain that $P(D^c|E) < P(D^c|E^c)$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item Combining this inequality with the relation in Odds Ratio equation, we see that $OR > RR$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \end{enumerate}

    \item the odds ratio and relative risk are equal to each other when $RR = 1$ (or $OR = 1$).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds ratio is often considered more complex than the relative risk, in particular because of the simplicity of interpretation of the relative risk.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The odds ratio is, however, more frequently used in practice than the relative risk. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item An important reason for this is that the odds ratio is symmetric in exposure $E$ and outcome $D$.
        If the roles of the exposure and outcome are interchanged the odds ratio does not change, but the relative risk does.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
        
    \end{enumerate}
\end{enumerate}



















\section{Simpson’s Paradox}

\begin{enumerate}
    \item Simpson’s Paradox is when a trend or relationship that appears in separate groups of data reverses or disappears when the groups are combined.
    \hfill \cite{common/online/chatgpt}

    \item In other words:
    \begin{enumerate}
        \item A conclusion that seems true when you look at each group individually
        \hfill \cite{common/online/chatgpt}

        \item Can be completely misleading or even opposite when you combine the data.
        \hfill \cite{common/online/chatgpt}
    \end{enumerate}

    \item Why does it happen? Because of confounding variables — something else (like unequal group sizes) is influencing the results.
    \hfill \cite{common/online/chatgpt}

    \item Simpson demonstrated that the association between $D$ and $E$ in the collapsed contingency table is preserved in the two separate contingency tables for $C$ and $C^c$ whenever one or both of the following restrictions hold true:
    \[
        P (D \cap  E \cap  C) P (D \cap  E^c \cap  C^c) = P (D \cap  E^c \cap  C) P (D \cap  E \cap  C^c)
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]
    \[
        P (D \cap  E \cap  C) P(D^c \cap  E \cap  C^c)= P(D^c \cap  E \cap  C)P(D \cap  E \cap  C^c)
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \]

    \begin{enumerate}
        \item The first equation implies that the odds ratio for having the exposure $E$ for the presence or absence of $C$ in the outcome group $D$ is equal to one, i.e.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \[
            OR_{EC|D} = \dfrac{P (E|C, D)[1 - P (E|C^c, D)]}{P (E|C^c, D)[1 - P (E|C, D)]} = 1
            \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \]
        Thus $E$ and $C$ must be independent in the outcome group $D$, which means that $P (E \cap C|D) = P (E|D) P (C|D)$.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}

        \item The second equation implies that the odds ratio for the outcome $D$ in the presence or absence of $C$ in the exposed group $E$ is equal to one, i.e.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \[
            OR_{DC|E} = \dfrac{P (D|C, E)[1 - P (D|C^c, E)]}{P (D|C^c, E)[1 - P (D|C, E)]} = 1
            \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
        \]
        Thus this means that $D$ and $C$ are independent in the exposed group $E$, which means that $P (D \cap C|E) = P (D|E) P (C|E)$.
        \hfill \text{\cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}}
    \end{enumerate}

    \item If the two independence requirements are violated, the event C is called a \textbf{confounder}.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item To say that C is not a confounder, we would want:
        \hfill \cite{common/online/chatgpt}
        \begin{enumerate}
            \item C is independent of E (i.e., the confounder is not related to the exposure): $C\perp E$
            \hfill \cite{common/online/chatgpt}

            \item C is independent of D given E (i.e., once we account for exposure, the confounder does not affect disease): $C \perp D | E$
            \hfill \cite{common/online/chatgpt}
        \end{enumerate}

        \item If C is related to both E and D — i.e.,
        \hfill \cite{common/online/chatgpt}
        \begin{enumerate}
            \item C affects the likelihood of being exposed (E), and
            \hfill \cite{common/online/chatgpt}

            \item C also affects the chance of getting the disease (D) independently of E,
            \hfill \cite{common/online/chatgpt}
        \end{enumerate}
        Then the independence assumptions are violated. Therefore, C is a confounder — because it distorts the observed relationship between E and D.
        \hfill \cite{common/online/chatgpt}
    \end{enumerate}
\end{enumerate}

\textbf{Example}:

\begin{customArrayStretch}{1.2}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        \multirow{2}{*}{Exposure} & \multicolumn{2}{c|}{Kidney Stones Outcome $\leq$ 2cm ($C$)} & \multirow{2}{*}{Total}\\
        \cline{2-3}
        & Removal ($D$) & No Removal ($D^c$) & \\
        \hline
        Nephrolithotomy ($E$) & 234 & 36 & 270 \\ \hline
        Open Surgery($E^c$) & 81 & 6 & 87 \\ \hline
        Total & 315 & 42 & 357 \\ 

        \hline \hline

        \hline
        \multirow{2}{*}{Exposure} & \multicolumn{2}{c|}{Kidney Stones Outcome $>$ 2cm ($C^c$)} & \multirow{2}{*}{Total}\\
        \cline{2-3}
        & Removal ($D$) & No Removal ($D^c$) & \\
        \hline

        Nephrolithotomy ($E$) & 55 & 25 & 80 \\ \hline
        Open Surgery($E^c$) & 192 & 71 & 263 \\ \hline
        Total & 247 & 96 & 343 \\ \hline
    \end{tabular}
    \caption{
        $2 \times 2$ contingency table for removal of kidney stones and two surgical treatments by size of kidney stones.
        \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    }
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        \multirow{2}{*}{Exposure} & \multicolumn{2}{c|}{Kidney Stones Outcome} & \multirow{2}{*}{Total}\\
        \cline{2-3}
        & Removal ($D$) & No Removal ($D^c$) & \\
        \hline

        Nephrolithotomy ($E$) & 289 & 61 & 350 \\ \hline
        Open surgery ($E^c$) & 273 & 77 & 350 \\ \hline
        Total & 562 & 138 & 700 \\ \hline
    \end{tabular}
    \caption{
        $2 \times 2$ contingency table for removal of kidney stones and two surgical treatments
        \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    }
\end{table}
\end{customArrayStretch}


\begin{enumerate}
    \item $RR = (289/350)/(273/350) = 1.0586$.  This means that percutaneous nephrolithotomy increases the “risk” of successful removal of the kidney stones with respect to open surgery.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item $RR_{\leq 2} = 0.9309$ and $RR_{>2} = 0.9417$. it seems that open surgery has a higher success of kidney stone removal than percutaneous nephrolithotomy for both small and large stones. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item This is a contradiction.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}










