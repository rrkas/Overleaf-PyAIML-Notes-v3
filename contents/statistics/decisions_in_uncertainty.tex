\chapter{Decisions in Uncertainty}


\section{Bootstrapping}

\begin{enumerate}
    \item  The bootstrap provides a very general way to obtain a quantification of the uncertainty of an estimator.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item a larger sample decreases the variance of an estimator (i.e., the estimator becomes more precise).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item when estimating a population mean or difference in population means, we find that a smaller population variance leads to a smaller variance of the estimator.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  The bootstrap has these exact same properties and is easy to carry out for many sample statistics; it provides a first entry into making decisions regarding populations based on sample data.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  informally, it is quite clear that a large random sample and a relatively small variance of the estimator $\hat{\theta}$ should both increase our confidence regarding statements we can make about the population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

\end{enumerate}

\subsection{Basic Idea of Bootstrap}

\begin{enumerate}
    \item Given a (random) sample of size n from some population with distribution function $F_X$ , we frequently set out to obtain an estimate of a population parameter $\theta = T (x)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Our point estimate of the parameter of interest is often what is called the “plug-in estimator” for $\theta$: $\hat{\theta} = T (x_1, \cdots , x_n )$; i.e., it is the statistic of interest calculated on the sample data $x_1, \cdots , x_n$ .
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we are interested in the distribution function of $\hat{\theta}$ over repeated samples.
    We are interested in $F_{\hat{\theta}}$ as it gives us information about the variability of our estimate over repeated samples.
    Depending on the statistic involved, the sampling plan, and the assumptions one is willing to make about $F_X$ or $f_X$ , we might be able to analytically derive $F_{\hat{\theta}}$.
    However, obtaining $F_{\hat{\theta}}$ in general (or properties thereof) can be challenging.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The bootstrap addresses the problem of deriving $F_{\hat{\theta}}$ using the power of computer simulation.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item if we have a good estimate of the population distribution function $F_X$ , i.e., $\hat{F}_X$ , we can simply program a computer to obtain $M$ samples (by \textbf{simulation}) of size $n$ from $\hat{F}_X$ using the same sampling plan that we have used to collect our initial sample.
    If $\hat{F}_X$ is close to $F_X$ it does not really matter if we draw from $\hat{F}_X$ or $F_X$ .
    As long as our estimate of $\hat{F}_X$ is close to the true $F_X$ , our samples of $\hat{F}_{\hat{\theta}}$ can be used to approximate properties of $F_{\hat{\theta}}$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item On each sample $m = 1, \cdots , M$ we can subsequently compute the statistic of interest, $\hat{\theta}^{(1)} , \cdots , \hat{\theta}^{(M)}$ which themselves serve as approximate samples from $F_{\hat{\theta}}$ (approximate as we are using $\hat{F}_X $, and thus we obtain samples from $\hat{F}_{\hat{\theta}}$).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item standard error of a statistic can simply be computed by computing the standard deviation of the M samples of the statistic of interest
    $
        \hat{SE}(\hat{\theta})
        = \sqrt{\dfrac{\dsum^M_{m=1} \dParenBrac{\hat{\theta}^{(m)}-\bar{\theta}}^2}{M-1}}
    $
    where
    $
        \bar{\theta} = \dfrac{1}{M} \dsum^M_{m=1} \hat{\theta}^{(m)}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we are interested in the variability of our estimator over differently obtained samples, we should adopt our bootstrapping procedure accordingly.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Although the bootstrap is appealing as it allows one to quantify the uncertainty for virtually any statistic—by simply replacing tedious analytical work with simple computer operations—one should always be careful: for complex sampling schemes and complex population distributions, $\hat{F}_X$ , or the resulting $M$ bootstrap samples of the statistic of interest, might not provide a good quantification of the uncertainty associated with $\hat{\theta}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item We can use $\hat{F}_X$ , in combination with computer simulation, to generate bootstrap samples $m = 1, \cdots , M$ and compute $\hat{\theta}(m)$ for arbitrary statistics $T $.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Steps}:
    \begin{enumerate}
        \item we estimate $F_X$ based using our sample $x_1, \cdots , x_n$ . This gives us $\hat{F}_X $.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item we obtain $M$ random samples from $\hat{F}_X$ (each of size $n$), on which we computed our bootstrap estimates of the statistic of interest $\hat{\theta}^{(1)} , \cdots, \hat{\theta}^{(M)}$ which we regard as (approximate) samples from $F_{\hat{\theta}}$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item \textbf{Disadvantages}:
    \begin{enumerate}
        \item it might be the case that $\hat{F}_X$ is a very poor estimate of $F_X$ .
        This is often the case when $n$ is small, but it might also be caused by the fact that the original sample $x_1, \cdots , x_n$ is not obtained through simple random sampling (resampling with replacement).
        If the latter is the case, the sampling scheme that was used should be taken into consideration when computing $\hat{F}_X$.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item the sampling scheme implemented in the second step (resampling step) should mimic the sampling scheme that was originally used: if the $M$ bootstrap samples are generated using a different sampling scheme than the sampling scheme of interest, $F_{\hat{\theta}}$ might not be properly approximated by the M bootstrap samples.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item While the bootstrap provides an easy way of quantifying uncertainty that we can use to make decisions, it is hard in general to make statements about the \textbf{quality} of these decisions.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

\subsubsection{Non-Parametric Bootstrap}

    \item no parametric assumptions regarding $F_X$ are made in this procedure
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  The simplest bootstrap approach for obtaining $\hat{F}_X$ is to simply use the empirical distribution function: the original samples $x_1, \cdots , x_n$ in our sample can be used to construct a discrete approximation of $F_X$ by simply giving each unique value $v_i$ in $x_1, \cdots , x_n$ probability $\dfrac{1}{n}$ .
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

\subsubsection{Parametric Bootstrap}

    \item in the parametric bootstrap $\hat{F}_X$ is assumed to be of a certain form (e.g., it is assumed to be normal), and plug-in estimates for its parameters (e.g., $\hat{\mu}$ and $\hat{\sigma}^2$ in the normal case) are used to estimate $F_X$ .
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If the assumptions are correct, the parametric bootstrap is preferable over the non-parametric bootstrap.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

\end{enumerate}






\section{Hypothesis testing}

\begin{enumerate}
    % \item  make binary decisions
    % \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{Test Statistics} ($t$): A single number that summarizes the sample data used to conduct the test hypothesis.
    \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

    \item $p$-\textbf{value}: Probability of observing a test statistics.
    \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}


    \item without making any assumptions regarding the sampling process and/or the population distributions involved, it is practically impossible to say anything about the population based on sample data with full certainty.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Hypothesis testing provides a method for making binary decisions that, in many instances, does give us clear quantitative statements about the quality of the decisions we make.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Within hypothesis testing the general setup is as follows: we state our decision problem as a choice between two competing hypotheses regarding the population, often called the \textbf{null hypothesis} $H_0 $, and the \textbf{alternative hypothesis} $H_a $.
    Given that $H_0$ and $H_a$ are complementary, one of the two \textbf{must} be true in the population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item \textbf{Null Hypothesis} ($H_0$): A statement of no change and is 0 assumed true until evidence indicates otherwise.
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

        \item \textbf{Alternate Hypothesis} ($H_a$): A statement that the researcher is trying to find evidence to support
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}
    \end{enumerate}

    \item The subsequent rationale of hypothesis testing is that we assume that the null hypothesis is true and that we gather \textbf{sufficient evidence} to demonstrate that it is not true.
    It defines sufficient evidence such that the probability of making a type 1 error is \textbf{at most} $\alpha$.
    The level $\alpha$ is called the \textbf{significance level} and it is the maximal allowable probability of rejecting the null hypothesis when the null hypothesis is actually true.
     It is often set equal to a value of $\alpha = 0.05$ or $\alpha = 0.01$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  Thus the \textbf{goal} of hypothesis testing is to \textbf{reject the null hypothesis} on the basis of sufficient and well-collected data.
    We will decide that either $H_0$ is rejected (thus $H_a$ must be true) or is not rejected (thus there is no or not enough evidence to demonstrate that $H_0$ is false).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}


\begin{figure}[H]
    \centering
    \includegraphics[
        width=\linewidth,
        height=5cm,
        keepaspectratio,
    ]{images/statistics/hypothesis-testing-errors.png}
    \caption{
        Types of errors in hypothesis testing
        \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    }
\end{figure}

    \item 4 situations:
    \begin{enumerate}
        \item {[\textbf{true positive}]} we do not reject $H_0$ when $H_0$ is true in the population
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item {[\textbf{true negative}]} we reject $H_0 $, and subsequently accept $H_a $, when $H_a$ is true in the population
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item {[\textbf{false positive}]} we reject $H_0$ while in reality it is true, aka false positive or type 1 error.
        The probability of a type 1 error is associated with the level $\alpha$.
        The $\alpha$ is used as a maximal allowable type 1 error for a decision rule.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item {[\textbf{false negative}]} we do not reject $H_0$ , while in actuality $H_a$ is true, aka type 2 error.
        The probability of a type 2 error is associated with the level $\beta$.
        The value $\beta$ is used a maximal allowable type 2 error.
        One minus the type 2 error ($1-\beta$) is called the \textbf{power} of the binary decision rule.
        It indicates how likely the null hypothesis is rejected when the alternative hypothesis is true.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}

    \item it is easy to create a decision procedure that has a type 1 error probability equal to zero:
    if we simply never reject $H_0$ —in this case basically we state that there is never sufficient evidence to reject $H_0$ —we will never make a type 1 error.
    While this decision procedure does control the type 1 error, it is clearly not very useful, as the power of this decision rule is zero: we never accept the alternative hypothesis when it is true.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the procedure of hypothesis testing aims to be less conservative (e.g., it will reject the null hypothesis sometimes but not too often when it would be true).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item \textbf{One tailed test}: Test statistics falls into one specified tail of its sampling distribution
    \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

    \item \textbf{Two tailed test}: Test statistics can falling into either tail of its sampling distribution
    \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

    \item Steps to Significance Testing:
    \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}
    \begin{enumerate}
        \item Define $H_0$ and $H_a$
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

        \item Identify test, $\alpha$, find critical value, test statistics
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

        \item Construct acceptance/rejection regions
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

        \item Calculate test statistics
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

        \begin{enumerate}
            \item Critical value approach: Determine critical region
            \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

            \item p-value approach: Calculate p-value
            \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}
        \end{enumerate}

        \item Retain or reject the hypothesis
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}
    \end{enumerate}

    \item \textbf{Choosing a Statistical Test}:

\begin{figure}[H]
    \centering
    \includegraphics[
        width=\linewidth,
        height=7cm,
        keepaspectratio,
    ]{images/statistics/hypothesis-testing-desicion-tree.png}
    \caption{
        Choosing a Statistical Test: Decision Tree
        \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}
    }
\end{figure}

    \begin{enumerate}
        \item Categorical Data: Use Chi Square
        \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

        \item Sample size ($n$):
        \begin{enumerate}
            \item $n < 30$ and Population Variance is unknown - t-test
            \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

            \item $n < 30$ and Population Variance is known - z-test
            \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}

            \item $n > 30$ - z-test or t-test
            \hfill \cite{ctl.unm.edu/assets/docs/resources/hypothesis-testing-sheet.pdf}
        \end{enumerate}
    \end{enumerate}

    \item The z-test heavily depends on asymptotic theory and therefore requires \textbf{large sample sizes}.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the t-test can be used for small sample sizes but under the strict assumption of having collected data from a \textbf{normal distribution}.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Alternative approaches for the z-test and t-test have been developed that require fewer or no assumptions. These tests are referred to as \textbf{non-parametric tests}.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}


\begin{figure}[H]
    \centering
    \includegraphics[
        width=\linewidth,
        height=5.5cm,
        keepaspectratio,
    ]{images/statistics/one-sided_two-sided_t-test.png}
\end{figure}

The difference between one-sided and two-sided tests.
In both cases $H_0$ is not rejected if the observed test statistic $t_n$ falls outside of the rejection region $\alpha$.
However, for a one-sided test (top) the rejection region is located on one tail of the distribution (the blue area denoted $\alpha$).
Hence, only if $t_n > z_{1-\alpha}$ (in this case), and sufficiently large to fall within the rejection region, is $H_0$ rejected.
For a two-sided test the rejection region is split over the two tails of the distribution: hence $H_0$ can be rejected if $t_n$ is either sufficiently small or sufficiently large.
Finally, note that if $t_n > z_{1-\alpha}$—if this is the direction of the one-sided test—a one-sided test might reject $H_0 $, whereas the same $t_n$ might not lead to a rejection in the two-sided case, because the critical value for the two-sided test is larger than for the one-sided test
\hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

















\subsection{The One-Sided $z$-Test for a Single Mean ($H_0:\mu(f) \leq \mu_0$)}

\begin{enumerate}
    \item hypothesis on the population mean: $H_0 : \mu( f ) \leq \mu_0$ versus $H_a : \mu( f ) > \mu_0 $.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we assume that the sample is large enough to be able to use the normal distribution function as an approximation to the distribution function of the statistic that we are using to make a decision about the null hypothesis
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Let’s assume that we have collected a random sample $Y_1 , Y_2, \cdots , Y_n$ from the population.
    We may estimate the population mean $\mu ( f )$ with the sample average $\bar{Y}$ .
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item when $\bar{Y}$ is smaller or equal to $\mu_0$ the random variable $\bar{Y}$ seems to be in line with the null hypothesis $H_0 : \mu( f ) \leq \mu_0$ .
    In other words, there is no evidence that the \textbf{null hypothesis is false}.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Although the random variable does not suggest any conflict with the null hypothesis $H_0 : \mu( f ) \leq \mu_0 $, it \textbf{does not guarantee} that $\mu( f ) \leq \mu_0$ either.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If the population mean $\mu( f )$ were somewhat larger than $\mu_0 $, it might not be completely unlikely to observe a sample average still below $\mu_0$ due to the sampling (a type 2 error).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item When $\bar{Y}$ is larger than $\mu_0 $, we might start to believe that the null hypothesis is incorrect.
    However, when $\bar{Y}$ is just a little higher than $\mu_0$ this might not be very unlikely either, even when $\mu( f ) \leq \mu_0$ .
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item For instance, for any symmetric $f$ at $\mu( f ) = \mu_0 $, the probability that $\bar{Y}$ is larger than $\mu_0$ is equal to $0.5$.
    Only when the sample average $\bar{Y}$ is substantially larger—thus when there is sufficient evidence—than $\bar{Y}$ would we start to indicate that the null hypothesis $H_0 : \mu( f ) \leq \mu_0$ is unlikely to be true (although a type 1 error could be made here).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we want to find a criterion for the average $\bar{Y}$ such that the average can only be larger than this criterion with a probability that is at most equal to $\alpha$ when the null hypothesis is true.
    If we assume that this criterion is equal to $\mu_0 + \delta$, with $\delta > 0$, then the probability that $\bar{Y}$ is larger than $\mu_0 + \delta$ is given by
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.3cm]
    .\hfill
    $
        P( \bar{Y} > \mu _0 + \delta)
        = P\dParenBrac{\dfrac{ ( \bar{Y} - \mu ( f ))}{\sigma( f )/\sqrt{n}} > \dfrac{\mu _0 - \mu ( f ) + \delta}{\sigma/\sqrt{n}}}
        \approx 1 - \Phi\dParenBrac{\dfrac{\mu _0 - \mu ( f ) + \delta}{\sigma( f )/\sqrt{n}}}
    $
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.3cm]
    where $\Phi$ is the standard normal PDF.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Under the null hypothesis $H_0 : \mu( f ) \leq \mu_0$ this probability is the type 1 error and it is maximized when $\mu( f ) = \mu_0$ .
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item if we deliberately set $\mu( f ) = \mu_0$ to maximize the type 1 error, the probability $P ( \bar{Y} > \mu_0 + \delta)$ is given by $1 - \Phi(\delta\sqrt{n}/\sigma)$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item When we choose $\delta$ equal to $\delta = \dfrac{z_{1-\alpha} \sigma( f )}{\sqrt{n}}$, the probability becomes $P ( \bar{Y} > \mu_0 + \delta) \approx \alpha$.
    When $\bar{Y} > \mu_0 + \dfrac{z_{1 - \alpha} \sigma( f )}{\sqrt{n}}$ the probability of rejecting the null hypothesis is at most $\alpha$—and hence we have defined sufficient evidence by the criterion $\mu0 + \dfrac{z_{1 - \alpha} \sigma( f )}{\sqrt{n}}$ for the null hypothesis $H_0 : \mu( f ) \leq \mu_0$ using the statistic $\bar{Y} $.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The null hypothesis could still be true, but that it is just bad luck to have observed such an unlikely large average under the null hypothesis.
    However, we know that this probability of having bad luck is less than or equal to $\alpha$ and therefore we accept making this potential type 1 error.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we cannot use the criterion ¯$Y > \mu_0 + \dfrac{z_{1-\alpha} \sigma( f )}{\sqrt{n}}$ directly, as it depends on the population standard deviation $\sigma( f )$, which is generally not known.
    We could estimate $\sigma( f )$ using sample standard deviation $S = \sqrt{\dfrac{1}{n-1} \dsum^n_{i=1} (Y_i - \bar{Y} )^2}$
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If the sample size is large enough, the probability $P\dParenBrac{\bar{Y} > \mu_0 + \dfrac{z_{1-\alpha} S}{\sqrt{n}}}$ would still be close to $\alpha$.
    When the sample size is large enough, we may reject the null hypothesis in practice when $\bar{Y} > \mu_0 + \dfrac{z_{1-\alpha} S}{\sqrt{n}}$ or in other words when the \textbf{asymptotic test statistic} $T_n = \dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}}$ is larger than $z_{1-\alpha}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item For large sample sizes n, the null hypothesis $H_0 : \mu( f ) \leq \mu_0$ is tested with test statistic $T_n = \dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}}$ and the null hypothesis is rejected with significance level $\alpha$ when the test statistic is larger than the critical value $z_{1-\alpha}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The null hypothesis is not rejected when $T_n = \dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}} \leq z_{1-\alpha}$, since there would not be enough evidence to reject the null hypothesis with significance level $\alpha$ (i.e., the observed result is not unlikely enough under $H_0 $).
    This does not mean that the null hypothesis is true.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item the asymptotic approach for testing $H_0 : \mu( f ) \leq \mu_0$ will work for most population densities $f_X$ whenever the sample size is large enough.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

% \subsubsection{p-value}

    \item The p-value is the probability that the observed test statistic $t_n$ and more extreme observations would occur under the null hypothesis.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item It measures how surprising the observed data is if the null hypothesis $H_0$ were true.
        \hfill \cite{common/online/chatgpt}

        \item A small p-value means the observed test statistic is very unlikely under $H_0 $, giving evidence against $H_0$
        \hfill \cite{common/online/chatgpt}
    \end{enumerate}

    \item Calculating p-value:
    \begin{enumerate}
        \item test statistic: $T_n = \dfrac{\bar{Y} - \mu_0}{S / \sqrt{n}}$
        \hfill \cite{common/online/chatgpt}

        \item observed test statistic: $t_n = \dfrac{\bar{Y} - \mu_0}{S / \sqrt{n}}$
        \hfill \cite{common/online/chatgpt}

        \item Convert test statistic to a probability under $H_0$
        \hfill \cite{common/online/chatgpt}
        \begin{enumerate}
            \item If $H_0$ is true, then Tn approximately follows a standard normal distribution (for large $n$).
            \hfill \cite{common/online/chatgpt}

            \item The p-value is the tail probability of seeing $t_{obs}$ or something more extreme.
            \hfill \cite{common/online/chatgpt}
        \end{enumerate}

        \item Depending on the test type:
        \hfill \cite{common/online/chatgpt}
        \begin{enumerate}
            \item Right-tailed test ($H_0:\mu\leq\mu_0$): $p=P(Z\geq t_{obs})=1-\Phi(t_{obs})$
            \hfill \cite{common/online/chatgpt}

            \item Left-tailed test ($H_0:\mu\geq\mu_0$): $p=P(Z\leq t_{obs})=\Phi(t_{obs})$
            \hfill \cite{common/online/chatgpt}

            \item Two-tailed test ($H_0:\mu= \mu_0$): $p = 2 \cdot \min(\Phi(t_{obs}), 1 - \Phi(t_{obs}))$
            \hfill \cite{common/online/chatgpt}
        \end{enumerate}
        Here $\Phi$ is the cumulative distribution function (CDF) of the standard normal.
        \hfill \cite{common/online/chatgpt}
    \end{enumerate}

\end{enumerate}




\subsection{The Two-Sided z-Test for a Single Mean ($H_0:\mu(f)=\mu_0$)}

\begin{enumerate}
    \item The null hypothesis is formulated as $H_0 : \mu( f ) = \mu_0$ and the alternative hypothesis is $H_a : \mu( f )  = \mu_0 $.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item We would reject the null hypothesis when either $T_n = \dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}}$ is large positive or large negative.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If we still keep our significance level at $\alpha$, we will reject the null hypothesis $H_0 : \mu( f )  = \mu_0$ when $\dfrac{\bar{Y} - \mu0}{S/\sqrt{n}} > z_{1-\alpha/2} $, with $z_{1-\alpha/2}$ the $1 - \alpha/2$ quantile of the standard normal distribution (when using a normal approximation), or when $\dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}} < -z_{1-\alpha/2} $.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we would reject when $\dfrac{\dabs{\bar{Y} - \mu_0}}{S/\sqrt{n}} > z_{1-\alpha/2} $, with $\dabs{x}$ the absolute value of $x$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item we reject this hypothesis when the test statistic is sufficiently small or sufficiently large.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item To control type 1 error, we therefore split up our rejection region in two: this is why we work with $z_{1-\alpha/2}$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  two-sided tests, while used very often in practice, are less useful when sample sizes are very large: with a very large $n$, the standard error of a test statistic will become small, and we eventually will reject the null hypothesis in most cases.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item confidence interval contains the statistic we just discussed for hypothesis testing. 
    In the case that $\mu_0$ is not contained in the confidence interval, either $\dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}} > z_{1-\alpha/2}$ or $\dfrac{\bar{Y} - \mu_0}{S/\sqrt{n}} < -z_{1-\alpha/2}$ would have occurred.
    Thus the null hypothesis $H_0 : \mu( f ) = \mu_0$ would be rejected if $\mu_0$ is not contained in $\dSquareBrac{\bar{Y} - \dfrac{z_{1-\alpha/2} S}{\sqrt{n}}, \bar{Y} + \dfrac{z_{1-\alpha/2} S}{\sqrt{n}}}$. Thus confidence intervals relate directly to two-sided hypothesis tests.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If the null hypothesis is included in the confidence interval, we do not have sufficient evidence to reject it.
    If the confidence interval lies fully outside of the null hypothesis, there is sufficient evidence to reject the null hypothesis. 
    Theoretically (and asymptotically) using confidence intervals for testing will lead to the same type 1 error probabilities.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The bootstrap approach is an analytical approach to obtain confidence intervals for certain statistics may be an alternative to generate confidence intervals (instead of using asymptotic theory).
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
\end{enumerate}



\subsection{Non-Parametric Test: Mann–Whitney U Test for Two Independent Samples ($H_0 : P(Y_1 > Y_2) = P(Y_1 < Y_2)$)}

\begin{enumerate}
    \item  It tests the null hypothesis $H_0 : P(Y_1 > Y_2) = P(Y_1 < Y_2)$, with $Y_1$ a random draw from the first population and $Y_2$ a random draw from the second population.
    The null hypothesis is equivalent to $H_0 : P(Y_1 > Y_2) = 0.5$.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  If the null hypothesis is false, it is more likely to observe larger values in one of the populations with respect to the other population. 
    Thus one population is \textbf{stochastically greater} than the other population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item If we now assume that the distribution function $F_2$ for the second population is given by $F_2(y) = F_1(y + \delta)$, with $F_1$ the distribution function of the first population and $\delta$ a (shift) parameter, the null hypothesis $H_0 : P(Y_1 > Y_2) = P(Y_1 < Y_2)$ implies that the medians of the two populations must be equal (i.e., $\delta = 0$). 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Only in this somewhat restrictive formulation of population distributions, the Mann–Whitney U test investigates whether the two populations have equal medians.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Mann–Whitney U test statistic: 
    \colorbox{yellow}{$
        U = 
        \dsum^{n_1} _{i=1} 
        \dsum^{n_2} _{j=1} 
        [1_{(Y_{2, j} <Y_{1,i} )} + 0.5 \cdot 1_{(Y_{1,i} =Y_{2, j} )}]
    $}
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \\[0.2cm]
    with $1_A$ the indicator function equal to $1$ if $A$ is true and zero otherwise.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  It represents the number of pairs $(Y_{1,i} , Y_{2, j} )$ of observations from the two populations for which the first population provides a larger value than the second population. 
    If there are ties (i.e., $Y_{1,i} = Y_{2, j}$ ), we cannot determine which population provides the larger value; thus this pair only contributes half to the total number of observations for which the first population is larger than the second population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item  If the variable $Y _h$ is continuous, we should not observe any ties if we use enough decimal places in the recording of the values.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item The Mann–Whitney U test only makes use of the \textbf{ordering} of the observations. 
    The total number of \textbf{pairs} for which the comparison between the two populations is made is equal to $n_1n_2 $. 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item Each observation of the first sample is compared to each observation in the second sample. 
    Thus the statistic $\dfrac{U}{n_1n_2}$ is an estimator of the probability $P(Y_1 > Y_2)$. 
    When $\dfrac{U}{n_1n_2}$ is away from $0.5$ or in other words, when U is away from $0.5n_1n_2$ , the null hypothesis $H_0 : P(Y_1 > Y_2) = 0.5$ is rejected and one population is considered stochastically greater than the other population.
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

    \item To determine whether the $U$ statistic is away from $0.5n_1n_2 $, we will make use of \textbf{asymptotic theory}, as we did with the $z$-test for means. 
    If the sample sizes $n_1$ and $n_2$ are large enough, the Mann–Whitney U statistic is approximately normally distributed with mean $\mu_U$ and variance $\sigma^2_U$ . 
    \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \begin{enumerate}
        \item Under the null hypothesis $H_0 : P(Y_1 > Y_2) = 0.5$, mean $\mu_U = 0.5n_1n_2$ .
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item The variance is then equal to $\sigma^2 _U = \dfrac{n_1n_2(n_1 + n_2 + 1)}{12}$, but only when there are \textbf{no ties}.
        If there are ties, a correction to this variance is required to make the variance smaller. 
        The ties reduce the variability in $U $.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
        
        \item To test the null hypothesis $H_0 : P(Y_1 > Y_2) = 0.5$ we calculate the standardized value $Z = \dfrac{U - 0.5n_1n_2}{\sqrt{n_1n_2(n_1 + n_2 + 1)/12}}$ and compare this with the quantiles $z_{\alpha/2}$ and $z_{1-\alpha/2}$ of the standard normal distribution. 
        If $\dabs{Z} > z_{1-\alpha/2}$ we reject the null hypothesis.
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}

        \item 
        \hfill \cite{statistics/book/Statistics-for-Data-Scientists/Maurits-Kaptein}
    \end{enumerate}
\end{enumerate}


















